{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPEPf45KVaurCsFMZlyTtPj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","active_directory = '/content/drive/MyDrive/Desktop/DP_Finetuning_Harnet_Submission'\n","os.chdir(active_directory)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eymmo6y9JD7u","executionInfo":{"status":"ok","timestamp":1750893770333,"user_tz":-120,"elapsed":1203,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"9007a34b-7f3d-4992-f086-af472b38945b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## IMPORT LIBRARIES"],"metadata":{"id":"dzNlWhrLKslJ"}},{"cell_type":"code","source":["\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import json\n","import pickle\n","from warnings import filterwarnings\n","from pandas.errors import SettingWithCopyWarning\n","\n","filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n","filterwarnings('ignore', category=UserWarning)"],"metadata":{"id":"G6y8_dI6KsAB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## READ SUBJECT FILES"],"metadata":{"id":"8C8j3rF9Ki9p"}},{"cell_type":"markdown","source":["Each of the data-files contains 54 columns per row, the columns contain the following data:\n","\n","-  1 timestamp (s)\n","- 2 activityID (see II.2. for the mapping to the activities)\n","- 3 heart rate (bpm)\n","- 4-20 IMU hand\n","- 21-37 IMU chest\n","- 38-54 IMU ankle\n","\n","The IMU sensory data contains the following columns:\n","\n","- 1 temperature (°C)\n","- 2-4 3D-acceleration data (ms-2), scale: ±16g, resolution: 13-bit\n","-  5-7 3D-acceleration data (ms-2), scale: ±6g, resolution: 13-bit*\n","- 8-10 3D-gyroscope data (rad/s)\n","- 11-13 3D-magnetometer data (μT)\n","- 14-17 orientation (invalid in this data collection)"],"metadata":{"id":"QO9a5LuNLPdL"}},{"cell_type":"markdown","source":["16g acceleration hand -> indexes [4, 5, 6]\n","\n","timestamp -> [0]\n","\n","activity_id -> [1]"],"metadata":{"id":"LZP8FRWEL1tZ"}},{"cell_type":"code","source":["def read_subject_file(file_path:str)->pd.DataFrame:\n","    subject_name = file_path.split('/')[-1].rstrip('.dat')\n","    subject = pd.read_table(file_path, header=None, sep='\\s+')\n","    return subject, subject_name\n","\n","class DataProcessor:\n","  def __init__(self, subject_dataframe:pd.DataFrame, subject_name:str):\n","    self.subject_dataframe = subject_dataframe\n","    self.subject_name = subject_name\n","    self.acc_x_col = 'acc_x'\n","    self.acc_y_col = 'acc_y'\n","    self.acc_z_col = 'acc_z'\n","    self.timestamp = 'timestamp'\n","    self.activity_id = 'activity_id'\n","\n","  def _extract_data(self)->pd.DataFrame:\n","    \"\"\"\n","    Extract the relevant columns from the data\n","    \"\"\"\n","    # timestamp, activity_id, 16g_acc_x, 16g_acc_y, 16g_acc_z\n","    raw_data = self.subject_dataframe.iloc[:, [0, 1, 4, 5, 6]]\n","    raw_data.columns = [self.timestamp , self.activity_id , self.acc_x_col, self.acc_y_col, self.acc_z_col]\n","    return raw_data\n","\n","  def _handle_missing_values(self, data:pd.DataFrame)->pd.DataFrame:\n","    \"\"\"\n","    Used linear interpolation for the acceleration data\n","    \"\"\"\n","    data[self.acc_x_col] = data[self.acc_x_col].interpolate(method='linear', limit_direction='both')\n","    data[self.acc_y_col] = data[self.acc_y_col].interpolate(method='linear', limit_direction='both')\n","    data[self.acc_z_col] = data[self.acc_z_col].interpolate(method='linear', limit_direction='both')\n","    return data\n","\n","  def sorted_timestamps(self, data:pd.DataFrame)->pd.DataFrame:\n","    \"\"\"\n","    Sort the timestamps in ascending order\n","    \"\"\"\n","    data = data.sort_values(by=self.timestamp, ascending=True).reset_index(drop=True)\n","    return data\n","\n","\n","  def downsample_from_100_to_30hz(self, data:pd.DataFrame)->pd.DataFrame:\n","    \"\"\"\n","    Downsample the data from 100Hz to 30Hz\n","    \"\"\"\n","\n","    data_copy = data.copy()\n","    data_copy.set_index(self.timestamp, inplace=True)\n","    data_copy.index = pd.to_timedelta(data_copy.index, unit=\"s\")\n","    data_copy = data_copy.resample('0.0333s').agg({self.activity_id: lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n","                                        self.acc_x_col:\"mean\",\n","                                        self.acc_y_col:\"mean\",\n","                                        self.acc_z_col:\"mean\"}).reset_index()\n","    return data_copy\n","\n","\n","def activity_mapping(value:int):\n","  mapping_dict = {\n","        0: \"other\",\n","        1: \"lying\",\n","        2: \"sitting\",\n","        3: \"standing\",\n","        4: \"walking\",\n","        5: \"running\",\n","        6: \"cycling\",\n","        7: \"Nordic walking\",\n","        9: \"watching TV\",\n","        10: \"computer work\",\n","        11: \"car driving\",\n","        12: \"ascending stairs\",\n","        13: \"descending stairs\",\n","        16: \"vacuum cleaning\",\n","        17: \"ironing\",\n","        18: \"folding laundry\",\n","        19: \"house cleaning\",\n","        20: \"playing soccer\",\n","        24: \"rope jumping\"\n","   }\n","  return mapping_dict.get(value, 'Unknown')\n","\n","\n","def create_sliding_windows(dataframe, window_length_sec, sampling_rate_hz, step_duration_sec):\n","  import scipy.stats as stats\n","\n","\n","  N_FEATURES = 3\n","  window_length_samples = int(window_length_sec * sampling_rate_hz)\n","  step_duration_samples = int(step_duration_sec * sampling_rate_hz)\n","  windows = []\n","  labels = []\n","  for i in range(0, len(dataframe)-window_length_samples, step_duration_samples):\n","    x = dataframe['acc_x'].values[i: i + window_length_samples]\n","    y = dataframe['acc_y'].values[i: i + window_length_samples]\n","    z = dataframe['acc_z'].values[i: i + window_length_samples]\n","    window = np.array([x, y, z])\n","    label = dataframe['activity_id'][i: i + window_length_samples]\n","\n","    windows.append(window)\n","    labels.append(label)\n","\n","  windows = np.asarray(windows).reshape(-1, N_FEATURES, window_length_samples, )\n","  labels = np.asarray(labels)\n","\n","  return windows, labels\n","\n","\n","\n","def clean_window_labels(window_X, window_y):\n","  \"\"\"\n","  Remove the windows that have activity id = 0 ratio > 0.5\n","  \"\"\"\n","  clean_window_X = []\n","  clean_window_y = []\n","\n","  for i in range(len(window_y)):\n","    if np.sum(window_y[i] == 0) / len(window_y[i]) < 0.5:\n","      clean_window_X.append(window_X[i])\n","      clean_window_y.append(window_y[i])\n","\n","  clean_window_X = np.array(clean_window_X)\n","  clean_window_y = np.array(clean_window_y)\n","\n","  return clean_window_X, clean_window_y\n","\n","\n","def majority_voting(window_y):\n","  \"\"\"\n","  Most frequent activity id in a window\n","  \"\"\"\n","  from scipy import stats as st\n","\n","  major_window_y = st.mode(window_y, axis=1).mode\n","  return major_window_y\n","\n","\n","def reshaped_windows(window_X, window_y):\n","  \"\"\"\n","  Reshape the windows to fit the model (n_windows, n_features, n_timestamps)\n","  \"\"\"\n","  window_X = window_X.reshape(window_X.shape[0], window_X.shape[2], window_X.shape[1])\n","  window_y = window_y.reshape(window_y.shape[0], )\n","  return window_X, window_y\n","\n","\n","def activity_filter(window_X, window_y):\n","  \"\"\"\n","  Filtering the activities that everybody does. (1, 2, 3, 4, 12, 13, 16, 17)\n","  \"\"\"\n","  filtered_window_X = []\n","  filtered_window_y = []\n","\n","  valid_indices = (\n","            (window_y == 1) | (window_y == 2) | (window_y == 3) | (window_y == 4) | (window_y == 12) | (window_y == 13) | (window_y == 16) | (window_y == 17)\n","        )\n","\n","  filtered_window_X = window_X[valid_indices]\n","  filtered_window_y = window_y[valid_indices]\n","\n","  return filtered_window_X, filtered_window_y\n","\n"],"metadata":{"id":"jXkn2boTOIk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_dir = 'Protocol'\n","subject_raw_files = os.listdir(file_dir)"],"metadata":{"id":"9I_bcSQThePN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subject_arr_Xs = []\n","subject_arr_ys = []\n","subject_names = []\n","\n","window_length_sec = 10\n","step_duration_sec = 5\n","sampling_rate_hz = 30\n","overlap_ratio = round((100*(window_length_sec-step_duration_sec)/window_length_sec), 2)\n","\n","\n","print(f'WINDOW LENGTH in SAMPLES: {int(window_length_sec * sampling_rate_hz)}')\n","print(f'STEP DURATION in SAMPLES: {int(step_duration_sec * sampling_rate_hz)}')\n","print(f'OVERLAPPING WINDOW RATIO: {overlap_ratio}%')\n","\n","\n","for f in subject_raw_files:\n","  file_path = os.path.join(file_dir, f)\n","  subject, subject_name = read_subject_file(file_path)\n","  if subject_name != 'subject109':\n","    print('-----------------------------------------------------------')\n","    print(f'Data preprocessing is starting for {subject_name}...')\n","    processor = DataProcessor(subject, subject_name)\n","    raw_data = processor._extract_data()\n","\n","    subject_df = processor._handle_missing_values(raw_data)   # Missing axes values are filled by applying linear interpolation\n","    subject_df = processor.sorted_timestamps(subject_df)    # Update if the timestamps is not ascending\n","    downsampled_subject_df = processor.downsample_from_100_to_30hz(subject_df)    # Downsample the data from 100Hz to 30Hz\n","\n","    win_X, win_y = create_sliding_windows(downsampled_subject_df, window_length_sec=window_length_sec, sampling_rate_hz=sampling_rate_hz, step_duration_sec=step_duration_sec)\n","    clean_win_X, clean_win_y = clean_window_labels(win_X, win_y)    # Remove the windows that have activity id = 0 ratio > 0.5\n","    major_win_y = majority_voting(clean_win_y)    # Majority voting for the labels in a window\n","    filtered_window_X, filtered_window_y = activity_filter(clean_win_X, major_win_y)   # Filtering the activities that everybody does. (1, 2, 3, 4, 12, 13, 16, 17)\n","\n","    print(f'Final remaining shapes X: {filtered_window_X.shape}, y: {filtered_window_y.shape}')\n","\n","    subject_arr_Xs.append(filtered_window_X)\n","    subject_arr_ys.append(filtered_window_y)\n","    subject_names.append(subject_name)\n","\n","\n","all_subject_infos = list(zip(subject_names, subject_arr_Xs, subject_arr_ys))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"reuKBAhbR8MS","executionInfo":{"status":"ok","timestamp":1750893986270,"user_tz":-120,"elapsed":211522,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"6a5d8f87-83af-475b-e130-adb6d9cd1669"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WINDOW LENGTH in SAMPLES: 300\n","STEP DURATION in SAMPLES: 150\n","OVERLAPPING WINDOW RATIO: 50.0%\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject101...\n","Final remaining shapes X: (345, 3, 300), y: (345,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject102...\n","Final remaining shapes X: (373, 3, 300), y: (373,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject103...\n","Final remaining shapes X: (348, 3, 300), y: (348,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject104...\n","Final remaining shapes X: (364, 3, 300), y: (364,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject105...\n","Final remaining shapes X: (379, 3, 300), y: (379,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject106...\n","Final remaining shapes X: (359, 3, 300), y: (359,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject107...\n","Final remaining shapes X: (355, 3, 300), y: (355,)\n","-----------------------------------------------------------\n","Data preprocessing is starting for subject108...\n","Final remaining shapes X: (364, 3, 300), y: (364,)\n"]}]},{"cell_type":"code","source":["activity_dfs = []\n","\n","for sb_n, Xs, ys in all_subject_infos:\n","  print(f'Subject {sb_n}: {np.unique(ys, return_counts=True)}')\n","\n","  subject_activities = np.vectorize(activity_mapping)(ys)\n","  activity_counts = pd.Series(subject_activities).value_counts(normalize=True).reset_index()\n","  activity_counts.columns = ['Activity', 'Ratio']\n","  activity_counts['Ratio'] = activity_counts['Ratio'] * 100\n","  activity_counts['Subject'] = sb_n\n","\n","  activity_dfs.append(activity_counts)\n","\n","final_activity_df = pd.concat(activity_dfs, axis=0, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OojE4BvZXePd","executionInfo":{"status":"ok","timestamp":1750893986295,"user_tz":-120,"elapsed":18,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"c06febc2-b9bd-47cb-d999-95b976759aac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Subject subject101: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([55, 47, 43, 45, 33, 29, 46, 47]))\n","Subject subject102: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([47, 45, 51, 65, 34, 31, 42, 58]))\n","Subject subject103: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([44, 57, 41, 58, 21, 30, 41, 56]))\n","Subject subject104: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([47, 51, 50, 64, 33, 29, 40, 50]))\n","Subject subject105: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([48, 53, 45, 64, 29, 25, 49, 66]))\n","Subject subject106: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([46, 46, 49, 51, 27, 22, 42, 76]))\n","Subject subject107: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([51, 25, 51, 67, 36, 22, 43, 60]))\n","Subject subject108: (array([ 1,  2,  3,  4, 12, 13, 16, 17]), array([48, 46, 50, 63, 23, 19, 49, 66]))\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def get_pretrained_harnet(class_num, model_name = 'harnet10'):\n","    repo = 'OxWearables/ssl-wearables'\n","    model = torch.hub.load(repo, model_name, class_num=class_num, pretrained=True, force_reload=True)\n","    return model\n","\n","def train(model, train_loader, optimizer, criterion, epoch, device, is_dp=False, privacy_engine=None, delta=None):\n","  model.train()\n","  total_loss = 0\n","  correct = 0\n","  total_samples = 0\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","      data, target = data.to(device), target.to(device)\n","      optimizer.zero_grad()\n","      output = model(data)\n","      loss = criterion(output, target)\n","      loss.backward()\n","      optimizer.step()\n","      total_loss += loss.item() * data.size(0)\n","      pred = output.argmax(dim=1, keepdim=True)\n","      correct += pred.eq(target.view_as(pred)).sum().item()\n","      total_samples += data.size(0)\n","      if batch_idx % 10 == 0:\n","          if is_dp and privacy_engine:\n","              epsilon = privacy_engine.get_epsilon(delta)\n","              print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n","                    f\"({100. * batch_idx / len(train_loader):.0f}%)]\\t\"\n","                    f\"Loss: {loss.item():.6f}\\tEpsilon: {epsilon:.2f}\")\n","          else:\n","              print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n","                    f\"({100. * batch_idx / len(train_loader):.0f}%)]\\t\"\n","                    f\"Loss: {loss.item():.6f}\")\n","\n","  avg_loss = total_loss / total_samples\n","  accuracy = 100. * correct / total_samples\n","  print(f\"Epoch {epoch} - Training: Average loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","  return avg_loss, accuracy\n","\n","\n","def evaluate(model, test_loader, criterion, device):\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  all_preds = []\n","  all_targets = []\n","  with torch.no_grad():\n","      for data, target in test_loader:\n","          data, target = data.to(device), target.to(device)\n","          output = model(data)\n","          test_loss += criterion(output, target).item() * data.size(0)\n","          pred = output.argmax(dim=1, keepdim=True)\n","          correct += pred.eq(target.view_as(pred)).sum().item()\n","          all_preds.extend(pred.cpu().numpy())\n","          all_targets.extend(target.cpu().numpy())\n","  test_loss /= len(test_loader.dataset)\n","  accuracy = 100. * correct / len(test_loader.dataset)\n","  f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n","  print(f\"Evaluation set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n","        f\"({accuracy:.2f}%), F1-score: {f1:.4f}\\n\")\n","  return test_loss, accuracy, f1\n","\n","\n","class CustomScaler:\n","  \"\"\"\n","  A wrapper for scikit-learn's StandardScaler that handles both 2D and 3D NumPy arrays.\n","  For 3D data, it reshapes to 2D, scales, and then reshapes back.\n","  \"\"\"\n","  def __init__(self):\n","      self._scaler = StandardScaler()\n","      self._is_fitted = False\n","      self._original_input_dims = None\n","  def fit(self, data: np.ndarray):\n","      self._original_input_dims = data.ndim\n","      if self._original_input_dims == 2:\n","          self._scaler.fit(data)\n","      elif self._original_input_dims == 3:\n","          n_samples, n_timesteps, n_features = data.shape\n","          reshaped_data = data.reshape((n_samples * n_timesteps, n_features))\n","          self._scaler.fit(reshaped_data)\n","      else:\n","          raise ValueError(\"Input data must have 2 or 3 dimensions for scaling.\")\n","      self._is_fitted = True\n","      return self\n","  def fit_transform(self, data: np.ndarray) -> np.ndarray:\n","      self._original_input_dims = data.ndim\n","\n","      if self._original_input_dims == 2:\n","          scaled_data = self._scaler.fit_transform(data)\n","      elif self._original_input_dims == 3:\n","          n_samples, n_timesteps, n_features = data.shape\n","          # Reshape 3D data to 2D for scaling\n","          reshaped_data = data.reshape((n_samples * n_timesteps, n_features))\n","          scaled_reshaped_data = self._scaler.fit_transform(reshaped_data)\n","          # Reshape scaled data back to original 3D shape\n","          scaled_data = scaled_reshaped_data.reshape((n_samples, n_timesteps, n_features))\n","      else:\n","          raise ValueError(\"Input data must have 2 or 3 dimensions for scaling.\")\n","\n","      self._is_fitted = True\n","      return scaled_data\n","\n","  def transform(self, data: np.ndarray) -> np.ndarray:\n","    if not self._is_fitted:\n","      raise RuntimeError(\"Scaler has not been fitted. Call fit_transform first.\")\n","    if data.ndim != self._original_input_dims:\n","      raise ValueError(f\"Input data has {data.ndim} dimensions, but scaler was fitted on \"\n","                            f\"data with {self._original_input_dims} dimensions.\")\n","    if self._original_input_dims == 2:\n","        scaled_data = self._scaler.transform(data)\n","    elif self._original_input_dims == 3:\n","        n_samples, n_timesteps, n_features = data.shape\n","        reshaped_data = data.reshape((n_samples * n_timesteps, n_features))\n","        scaled_reshaped_data = self._scaler.transform(reshaped_data)\n","        scaled_data = scaled_reshaped_data.reshape((n_samples, n_timesteps, n_features))\n","    else:\n","        raise ValueError(\"Input data must have 2 or 3 dimensions.\")\n","\n","    return scaled_data\n","\n","def get_data_loader(X_test, y_test, BATCH_SIZE, shuffle=False):\n","  X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","  y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","  test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=shuffle)\n","  return test_loader\n","\n","class EarlyStopping:\n","  def __init__(self, patience=5, delta=0, verbose=False):\n","      self.patience = patience\n","      self.delta = delta\n","      self.best_score = None\n","      self.early_stop = False\n","      self.counter = 0\n","      self.best_model_state = None\n","      self.verbose = verbose # Added verbose\n","      self.best_epoch = 0 # To track the epoch of the best model\n","  def __call__(self, val_loss, model, epoch):\n","      score = -val_loss\n","      if self.best_score is None:\n","          self.best_score = score\n","          self.best_model_state = copy.deepcopy(model.state_dict())\n","          self.best_epoch = epoch # Store best epoch\n","      elif score < self.best_score + self.delta:\n","          self.counter += 1\n","          if self.verbose:\n","              print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","          if self.counter >= self.patience:\n","              self.early_stop = True\n","      else:\n","          self.best_score = score\n","          self.best_model_state = copy.deepcopy(model.state_dict())\n","          self.best_epoch = epoch # Store best epoch\n","          self.counter = 0\n","\n","  def load_best_model(self, model):\n","      if self.best_model_state:\n","          model.load_state_dict(self.best_model_state)\n","\n","\n","\n","def plot_distribution(arr:np.ndarray, title:str):\n","  pd.Series(arr).value_counts(normalize=True).plot(kind='bar')\n","  plt.title(title)\n","  plt.show()\n"],"metadata":{"id":"1-w4mckSNfRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fold_plot(fold_train_accuracies, fold_val_accuracies, val_subjects):\n","  import matplotlib.pyplot as plt\n","  import math\n","  # Get the list of train and val accuracies\n","  acc_list_zipped = list(zip(fold_train_accuracies, fold_val_accuracies, val_subjects))\n","\n","  # Get the total number of folds to plot\n","  num_folds = len(acc_list_zipped)\n","\n","  # --- Subplot Layout Calculation ---\n","  ncols = 2\n","  nrows = math.ceil(num_folds / ncols)\n","\n","  # --- Create the Figure and Subplots ---\n","  fig, axes = plt.subplots(nrows, ncols, figsize=(14, 5 * nrows))\n","  fig.suptitle('Training vs. Validation Accuracy Across Folds', fontsize=16, y=1.02)\n","  axes = axes.flatten()\n","\n","\n","  # --- Loop and Plot on Each Subplot ---\n","  for i, (tr_acc, vl_acc, vl_subjects) in enumerate(acc_list_zipped):\n","      ax = axes[i] # Get the current axis\n","      ax.plot(tr_acc, label='Train Accuracy', color='royalblue')\n","      ax.plot(vl_acc, label='Validation Accuracy', color='darkorange')\n","      ax.set_title(f'Fold: {i + 1} for  Validation Subjects {vl_subjects}')\n","      ax.set_xlabel('Epoch')\n","      ax.set_ylabel('Accuracy')\n","      ax.legend()\n","      ax.grid(True, linestyle='--', alpha=0.6)\n","\n","  # --- Clean Up and Display ---\n","  # If the number of folds is odd, the last subplot in the grid will be empty.\n","  # This loop hides any unused subplots.\n","  for i in range(num_folds, len(axes)):\n","      axes[i].axis('off')\n","\n","  # Adjusts subplot params so that subplots are nicely fit in the figure.\n","  fig.tight_layout(rect=[0, 0, 1, 0.98])\n","\n","  return fig\n","\n","\n","def plot_epochs(train_accs, val_accs, val_subj):\n","  import matplotlib.pyplot as plt\n","  epochs = np.arange(1, len(train_accs) + 1)\n","  fig, ax = plt.subplots()\n","  ax.plot(epochs, train_accs, label='Train Accuracy', color='royalblue')\n","  ax.plot(epochs, val_accs, label='Validation Accuracy', color='darkorange')\n","  ax.set_title(f'Training vs. Validation Accuracy for Validation Subjects {val_subj}')\n","  ax.set_xlabel('Epoch')\n","  ax.set_ylabel('Accuracy')\n","  ax.set_xticks(epochs)\n","  ax.legend()\n","  ax.grid(True, linestyle='--', alpha=0.6)\n","  return fig\n","\n","\n","def remove_module_prefix(state_dict):\n","    return {k.replace(\"_module.\", \"\"): v for k, v in state_dict.items()}\n"],"metadata":{"id":"lInWCAAv36dq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## FINE TUNING of CLASSIFIER HEADS (DP)\n","\n"],"metadata":{"id":"_LyIbpV_kgOC"}},{"cell_type":"code","source":["#%pip install opacus"],"metadata":{"id":"nj_qXZ8dR9eX","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HYPERPARAMETER TUNING\n","import datetime as dt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedGroupKFold, LeaveOneGroupOut\n","import torch.nn as nn\n","import copy\n","from itertools import product\n","from opacus import PrivacyEngine\n","from opacus.accountants.utils import get_noise_multiplier\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","reports_save = True\n","\n","label_encoder = LabelEncoder()\n","\n","X_combined = []\n","y_combined = []\n","subject_groups_combined = []\n","\n","mia_test_X = []\n","mia_test_y = []\n","test_groups_combined = []\n","mia_test_subjects = []\n","\n","for sb_name, Xs, ys in all_subject_infos:\n","  if sb_name not in ['subject102','subject103']:           # Test subject for membership inference attack as a non-member\n","    X_combined.extend(Xs)\n","    y_combined.extend(ys)\n","    subject_groups_combined.extend([sb_name] * len(ys))\n","  else:\n","    mia_test_subjects.append(sb_name)\n","    mia_test_X.extend(Xs)\n","    mia_test_y.extend(ys)\n","    test_groups_combined.extend([sb_name] * len(ys))\n","\n","\n","\n","X_combined = np.array(X_combined)\n","y_combined = np.array(y_combined)\n","subject_groups_combined = np.array(subject_groups_combined)\n","\n","label_encoder.fit(y_combined)\n","encoded_y_combined_raw = label_encoder.transform(y_combined)\n","\n","EPOCHS = 20\n","BATCH_SIZE_LIST = [32]\n","LRATE_LIST = [1e-3]\n","\n","\n","parameter_pairs = list(product(BATCH_SIZE_LIST, LRATE_LIST))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","N_CLASSES = len(label_encoder.classes_)\n","g_noise_multp = 1.0\n","C_value = 1.0\n","noise_scale = g_noise_multp * C_value\n","DELTA = 1e-5\n","\n","original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOlG4EzO-I2G","executionInfo":{"status":"ok","timestamp":1750894417186,"user_tz":-120,"elapsed":10812,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"d333f3cb-90b8-4fdc-9d9c-679eb302f2c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/OxWearables/ssl-wearables/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"stream","name":"stdout","text":["131 Weights loaded\n"]}]},{"cell_type":"code","source":["\n","\n","final_best_model = None\n","final_best_val_loss = float('inf')\n","final_best_val_acc = 0\n","final_best_val_f1 = 0\n","final_scaler = None\n","final_best_epoch = None\n","final_best_train_acc = None\n","final_best_train_loss = None\n","final_best_train_subjects = None\n","final_best_val_subjects = None\n","final_epsilon_spent = None\n","final_BATCH_SIZE = None\n","final_LRATE = None\n","\n","for BATCH_SIZE, LRATE in parameter_pairs:\n","  print('-------------------------------------------------------------------------------------------------')\n","  print(f'Batch size: {BATCH_SIZE}, Learning rate: {LRATE}')\n","  print('-------------------------------------------------------------------------------------------------')\n","\n","  fold_details = {\"Val Subjects\": [], \"Batch_Size\": BATCH_SIZE, \"Learning_Rate\": LRATE, \"Epoch_Results\": []}\n","\n","  # Leave One Group Out for inner fold\n","  inner_skf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n","  for fold, (train_index, val_index) in enumerate(inner_skf.split(X_combined, encoded_y_combined_raw, subject_groups_combined), start=1):\n","    print(f'Fold {fold} is starting ...')\n","    X_train, X_val = X_combined[train_index], X_combined[val_index]\n","    y_train, y_val = encoded_y_combined_raw[train_index], encoded_y_combined_raw[val_index]\n","    val_subjects = np.unique(subject_groups_combined[val_index])\n","    train_subjects = np.unique(subject_groups_combined[train_index])\n","    print(f'Validation subjects : {val_subjects}')\n","    print(f'Train subjects: {train_subjects}')\n","\n","\n","    scaler = CustomScaler()\n","    scaled_X_train = scaler.fit_transform(X_train)\n","    scaled_X_val = scaler.transform(X_val)\n","    train_data_loader = get_data_loader(scaled_X_train, y_train, BATCH_SIZE=BATCH_SIZE, shuffle=True)\n","    val_data_loader = get_data_loader(scaled_X_val, y_val, BATCH_SIZE=BATCH_SIZE, shuffle=False)\n","    model = copy.deepcopy(original_model)\n","    model.to(device)\n","    for name, param in model.named_parameters():\n","      if name.startswith('classifier'):\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = False\n","    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LRATE, weight_decay=0.01)\n","    criterion = nn.CrossEntropyLoss()\n","    early_stopping = EarlyStopping(patience=10, verbose=True)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n","    privacy_engine = PrivacyEngine(secure_mode=False)\n","    model, optimizer, train_data_loader = privacy_engine.make_private(\n","        module=model,\n","        optimizer=optimizer,\n","        data_loader=train_data_loader,\n","        noise_multiplier=g_noise_multp,\n","        max_grad_norm=C_value\n","        )\n","\n","    val_losses = []\n","    val_accuracies = []\n","    val_f1s = []\n","    train_losses = []\n","    train_accuracies = []\n","    epochs_list = []\n","    epoch_results = {\"epoch\": [], \"val_subjects\":'-'.join(val_subjects), \"noise_scale\":[], \"achieved_epsilon\": [],\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","    best_scaler = None\n","    best_epoch = None\n","    for epoch in range(1, EPOCHS + 1):\n","      train_loss, train_acc = train(model, train_data_loader, optimizer, criterion, epoch, device)\n","      val_loss, val_acc, val_f1 = evaluate(model, val_data_loader, criterion, device)\n","      scheduler.step(val_loss)\n","      train_losses.append(train_loss)\n","      train_accuracies.append(train_acc)\n","      val_losses.append(val_loss)\n","      val_accuracies.append(val_acc)\n","      val_f1s.append(val_f1)\n","      epochs_list.append(epoch)\n","\n","      a_epsilon = round(privacy_engine.get_epsilon(DELTA), 3)\n","      epoch_results[\"noise_scale\"].append(noise_scale)\n","      epoch_results[\"achieved_epsilon\"].append(a_epsilon)\n","      print(f'Epsilon Spent: {a_epsilon}, Noise_Scale: {noise_scale}')\n","      epoch_results[\"epoch\"].append(epoch)\n","      epoch_results[\"train_loss\"].append(train_loss)\n","      epoch_results[\"train_acc\"].append(train_acc)\n","      epoch_results[\"val_loss\"].append(val_loss)\n","      epoch_results[\"val_acc\"].append(val_acc)\n","      epoch_results[\"val_f1\"].append(val_f1)\n","      if val_loss < final_best_val_loss:\n","        final_best_val_loss = val_loss\n","        final_best_model = copy.deepcopy(model.state_dict())\n","        final_best_val_acc = val_acc\n","        final_best_val_f1 = val_f1\n","        final_scaler = copy.deepcopy(scaler)\n","        final_best_epoch = epoch\n","        final_best_train_acc = train_acc\n","        final_best_train_loss = train_loss\n","        final_best_train_subjects = train_subjects\n","        final_best_val_subjects = val_subjects\n","        final_epsilon_spent = a_epsilon\n","        final_BATCH_SIZE = BATCH_SIZE\n","        final_LRATE = LRATE\n","\n","\n","      early_stopping(val_loss, model, epoch)\n","      if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","    fold_details[\"Val Subjects\"].append(val_subjects)\n","    fold_details[\"Epoch_Results\"].append(epoch_results)\n","\n","\n","today_time = dt.date.today().strftime(\"%Y%m%d\")\n","vers = f'{window_length_sec}_{sampling_rate_hz}_{overlap_ratio}_{today_time}'\n","\n","\n","fold_details_df = pd.DataFrame(fold_details)\n","\n","noise_scale_vers = int(noise_scale)\n","\n","final_model_version = f'final_model_{noise_scale_vers}_{vers}.pth'\n","final_scaler_version = f'final_scaler_{noise_scale_vers}_{vers}.pkl'\n","final_model_df = pd.DataFrame({\"Final_Model_Version\":[final_model_version],\n","                               \"Final_Scaler_Version\":[final_scaler_version],\n","                               \"Final_Best_Val_Loss\":final_best_val_loss,\n","                               \"Final_Best_Val_Acc\":final_best_val_acc,\n","                               \"Final_Best_Val_F1\":final_best_val_f1,\n","                               \"Final_Best_Epoch\":final_best_epoch,\n","                               \"Final_Best_Train_Acc\":final_best_train_acc,\n","                               \"Final_Best_Train_Loss\":final_best_train_loss,\n","                               \"Final_Best_Train_Subjects\":'-'.join(final_best_train_subjects),\n","                               \"Final_Best_Val_Subjects\":'-'.join(final_best_val_subjects),\n","                               \"Final_Epsilon_Spent\":final_epsilon_spent,\n","                               \"Final_Batch_Size\":final_BATCH_SIZE,\n","                               \"Final_Learning_Rate\":final_LRATE,\n","                               })\n","\n","\n","\n","if reports_save:\n","  directory_name = 'attack_results_DP'\n","  os.makedirs(directory_name, exist_ok=True)\n","\n","  model_directory_name = 'attack_results_DP/final_models'\n","  os.makedirs(model_directory_name, exist_ok=True)\n","\n","\n","  torch.save(final_best_model, os.path.join(model_directory_name, final_model_version))\n","\n","\n","  with open(os.path.join(model_directory_name, final_scaler_version), 'wb') as f:\n","    pickle.dump(final_scaler, f)\n","\n","\n","  final_label_encoder_version = f'final_label_encoder_{noise_scale_vers}_{vers}.pkl'\n","  with open(os.path.join(model_directory_name, final_label_encoder_version), 'wb') as f:\n","    pickle.dump(label_encoder, f)\n","\n","  fold_details_df.to_csv(f'{directory_name}/fold_details_DP_CH_NS_{noise_scale_vers}_{vers}_Split_4-2-2.csv')\n","  final_model_df.to_csv(f'{directory_name}/final_model_df_DP_CH_NS_{noise_scale_vers}_{vers}_Split_4-2-2.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWnChRQ7-Ivm","executionInfo":{"status":"ok","timestamp":1750894482694,"user_tz":-120,"elapsed":57444,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"10ce1199-9bd6-49e5-874c-dea90725b836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------------------------------------------------\n","Batch size: 32, Learning rate: 0.001\n","-------------------------------------------------------------------------------------------------\n","Fold 1 is starting ...\n","Validation subjects : ['subject104' 'subject105']\n","Train subjects: ['subject101' 'subject106' 'subject107' 'subject108']\n","Train Epoch: 1 [0/1423 (0%)]\tLoss: 7.469398\n","Train Epoch: 1 [210/1423 (22%)]\tLoss: 6.490749\n","Train Epoch: 1 [600/1423 (44%)]\tLoss: 5.468999\n","Train Epoch: 1 [1140/1423 (67%)]\tLoss: 4.276531\n","Train Epoch: 1 [1240/1423 (89%)]\tLoss: 3.589302\n","Epoch 1 - Training: Average loss: 5.5608, Accuracy: 24.38%\n","Evaluation set: Average loss: 4.6077, Accuracy: 243/743 (32.71%), F1-score: 0.2473\n","\n","Epsilon Spent: 1.235, Noise_Scale: 1.0\n","Train Epoch: 2 [0/1423 (0%)]\tLoss: 3.525553\n","Train Epoch: 2 [320/1423 (22%)]\tLoss: 5.900445\n","Train Epoch: 2 [640/1423 (44%)]\tLoss: 3.040868\n","Train Epoch: 2 [840/1423 (67%)]\tLoss: 3.475111\n","Train Epoch: 2 [920/1423 (89%)]\tLoss: 3.662357\n","Epoch 2 - Training: Average loss: 3.6918, Accuracy: 37.27%\n","Evaluation set: Average loss: 3.4790, Accuracy: 272/743 (36.61%), F1-score: 0.3043\n","\n","Epsilon Spent: 1.535, Noise_Scale: 1.0\n","Train Epoch: 3 [0/1423 (0%)]\tLoss: 3.118705\n","Train Epoch: 3 [300/1423 (22%)]\tLoss: 3.799262\n","Train Epoch: 3 [500/1423 (44%)]\tLoss: 1.909597\n","Train Epoch: 3 [870/1423 (67%)]\tLoss: 1.418881\n","Train Epoch: 3 [1160/1423 (89%)]\tLoss: 3.072422\n","Epoch 3 - Training: Average loss: 2.7076, Accuracy: 47.44%\n","Evaluation set: Average loss: 3.0510, Accuracy: 323/743 (43.47%), F1-score: 0.3967\n","\n","Epsilon Spent: 1.776, Noise_Scale: 1.0\n","Train Epoch: 4 [0/1423 (0%)]\tLoss: 2.220327\n","Train Epoch: 4 [350/1423 (22%)]\tLoss: 2.455156\n","Train Epoch: 4 [540/1423 (44%)]\tLoss: 2.850693\n","Train Epoch: 4 [960/1423 (67%)]\tLoss: 2.279146\n","Train Epoch: 4 [1440/1423 (89%)]\tLoss: 2.491515\n","Epoch 4 - Training: Average loss: 2.2673, Accuracy: 54.15%\n","Evaluation set: Average loss: 2.7531, Accuracy: 369/743 (49.66%), F1-score: 0.4555\n","\n","Epsilon Spent: 1.986, Noise_Scale: 1.0\n","Train Epoch: 5 [0/1423 (0%)]\tLoss: 2.771258\n","Train Epoch: 5 [320/1423 (22%)]\tLoss: 2.186669\n","Train Epoch: 5 [600/1423 (44%)]\tLoss: 0.936412\n","Train Epoch: 5 [990/1423 (67%)]\tLoss: 2.038961\n","Train Epoch: 5 [1480/1423 (89%)]\tLoss: 3.367662\n","Epoch 5 - Training: Average loss: 2.1523, Accuracy: 61.14%\n","Evaluation set: Average loss: 2.6477, Accuracy: 384/743 (51.68%), F1-score: 0.4744\n","\n","Epsilon Spent: 2.177, Noise_Scale: 1.0\n","Train Epoch: 6 [0/1423 (0%)]\tLoss: 3.418757\n","Train Epoch: 6 [330/1423 (22%)]\tLoss: 1.676909\n","Train Epoch: 6 [560/1423 (44%)]\tLoss: 1.986284\n","Train Epoch: 6 [900/1423 (67%)]\tLoss: 2.516383\n","Train Epoch: 6 [1320/1423 (89%)]\tLoss: 2.576344\n","Epoch 6 - Training: Average loss: 2.0297, Accuracy: 64.87%\n","Evaluation set: Average loss: 2.4743, Accuracy: 381/743 (51.28%), F1-score: 0.4820\n","\n","Epsilon Spent: 2.353, Noise_Scale: 1.0\n","Train Epoch: 7 [0/1423 (0%)]\tLoss: 2.383849\n","Train Epoch: 7 [340/1423 (22%)]\tLoss: 1.733621\n","Train Epoch: 7 [620/1423 (44%)]\tLoss: 2.864273\n","Train Epoch: 7 [1110/1423 (67%)]\tLoss: 0.962517\n","Train Epoch: 7 [1080/1423 (89%)]\tLoss: 1.752412\n","Epoch 7 - Training: Average loss: 1.9850, Accuracy: 64.71%\n","Evaluation set: Average loss: 2.6641, Accuracy: 414/743 (55.72%), F1-score: 0.5316\n","\n","Epsilon Spent: 2.518, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 8 [0/1423 (0%)]\tLoss: 2.048997\n","Train Epoch: 8 [280/1423 (22%)]\tLoss: 1.329419\n","Train Epoch: 8 [680/1423 (44%)]\tLoss: 1.206214\n","Train Epoch: 8 [810/1423 (67%)]\tLoss: 3.396310\n","Train Epoch: 8 [1240/1423 (89%)]\tLoss: 1.603717\n","Epoch 8 - Training: Average loss: 1.9761, Accuracy: 68.30%\n","Evaluation set: Average loss: 2.5922, Accuracy: 413/743 (55.59%), F1-score: 0.5218\n","\n","Epsilon Spent: 2.674, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 9 [0/1423 (0%)]\tLoss: 1.943544\n","Train Epoch: 9 [340/1423 (22%)]\tLoss: 2.983023\n","Train Epoch: 9 [740/1423 (44%)]\tLoss: 1.926816\n","Train Epoch: 9 [930/1423 (67%)]\tLoss: 1.334553\n","Train Epoch: 9 [1280/1423 (89%)]\tLoss: 1.539800\n","Epoch 9 - Training: Average loss: 1.7109, Accuracy: 70.33%\n","Evaluation set: Average loss: 2.6125, Accuracy: 426/743 (57.34%), F1-score: 0.5441\n","\n","Epsilon Spent: 2.822, Noise_Scale: 1.0\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 10 [0/1423 (0%)]\tLoss: 1.861633\n","Train Epoch: 10 [270/1423 (22%)]\tLoss: 2.346467\n","Train Epoch: 10 [920/1423 (44%)]\tLoss: 0.569487\n","Train Epoch: 10 [1020/1423 (67%)]\tLoss: 1.761463\n","Train Epoch: 10 [1120/1423 (89%)]\tLoss: 0.856587\n","Epoch 10 - Training: Average loss: 1.8299, Accuracy: 70.24%\n","Evaluation set: Average loss: 2.5447, Accuracy: 434/743 (58.41%), F1-score: 0.5600\n","\n","Epsilon Spent: 2.964, Noise_Scale: 1.0\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 11 [0/1423 (0%)]\tLoss: 2.165311\n","Train Epoch: 11 [410/1423 (22%)]\tLoss: 2.131259\n","Train Epoch: 11 [820/1423 (44%)]\tLoss: 1.831176\n","Train Epoch: 11 [900/1423 (67%)]\tLoss: 0.981812\n","Train Epoch: 11 [1160/1423 (89%)]\tLoss: 1.022167\n","Epoch 11 - Training: Average loss: 1.6286, Accuracy: 72.11%\n","Evaluation set: Average loss: 2.7156, Accuracy: 432/743 (58.14%), F1-score: 0.5579\n","\n","Epsilon Spent: 3.101, Noise_Scale: 1.0\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 12 [0/1423 (0%)]\tLoss: 1.957340\n","Train Epoch: 12 [340/1423 (22%)]\tLoss: 1.349406\n","Train Epoch: 12 [500/1423 (44%)]\tLoss: 1.925453\n","Train Epoch: 12 [870/1423 (67%)]\tLoss: 1.178633\n","Train Epoch: 12 [1640/1423 (89%)]\tLoss: 1.340089\n","Epoch 12 - Training: Average loss: 1.7377, Accuracy: 69.75%\n","Evaluation set: Average loss: 2.6427, Accuracy: 426/743 (57.34%), F1-score: 0.5514\n","\n","Epsilon Spent: 3.232, Noise_Scale: 1.0\n","EarlyStopping counter: 6 out of 10\n","Train Epoch: 13 [0/1423 (0%)]\tLoss: 1.989370\n","Train Epoch: 13 [350/1423 (22%)]\tLoss: 1.460138\n","Train Epoch: 13 [580/1423 (44%)]\tLoss: 1.561733\n","Train Epoch: 13 [1080/1423 (67%)]\tLoss: 1.527201\n","Train Epoch: 13 [1080/1423 (89%)]\tLoss: 1.864600\n","Epoch 13 - Training: Average loss: 1.6443, Accuracy: 71.52%\n","Evaluation set: Average loss: 2.4794, Accuracy: 433/743 (58.28%), F1-score: 0.5644\n","\n","Epsilon Spent: 3.359, Noise_Scale: 1.0\n","EarlyStopping counter: 7 out of 10\n","Train Epoch: 14 [0/1423 (0%)]\tLoss: 1.415471\n","Train Epoch: 14 [240/1423 (22%)]\tLoss: 2.650622\n","Train Epoch: 14 [800/1423 (44%)]\tLoss: 2.060213\n","Train Epoch: 14 [780/1423 (67%)]\tLoss: 2.688918\n","Train Epoch: 14 [1520/1423 (89%)]\tLoss: 1.876031\n","Epoch 14 - Training: Average loss: 1.8458, Accuracy: 68.39%\n","Evaluation set: Average loss: 2.3790, Accuracy: 438/743 (58.95%), F1-score: 0.5645\n","\n","Epsilon Spent: 3.483, Noise_Scale: 1.0\n","Train Epoch: 15 [0/1423 (0%)]\tLoss: 1.566467\n","Train Epoch: 15 [260/1423 (22%)]\tLoss: 2.070249\n","Train Epoch: 15 [640/1423 (44%)]\tLoss: 1.209861\n","Train Epoch: 15 [1020/1423 (67%)]\tLoss: 0.497992\n","Train Epoch: 15 [1200/1423 (89%)]\tLoss: 1.814365\n","Epoch 15 - Training: Average loss: 1.7486, Accuracy: 70.13%\n","Evaluation set: Average loss: 2.3989, Accuracy: 432/743 (58.14%), F1-score: 0.5512\n","\n","Epsilon Spent: 3.603, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 16 [0/1423 (0%)]\tLoss: 2.075605\n","Train Epoch: 16 [300/1423 (22%)]\tLoss: 2.278601\n","Train Epoch: 16 [560/1423 (44%)]\tLoss: 1.825083\n","Train Epoch: 16 [960/1423 (67%)]\tLoss: 1.324282\n","Train Epoch: 16 [800/1423 (89%)]\tLoss: 0.961071\n","Epoch 16 - Training: Average loss: 1.6499, Accuracy: 70.79%\n","Evaluation set: Average loss: 2.4440, Accuracy: 436/743 (58.68%), F1-score: 0.5632\n","\n","Epsilon Spent: 3.72, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 17 [0/1423 (0%)]\tLoss: 1.063087\n","Train Epoch: 17 [310/1423 (22%)]\tLoss: 0.892187\n","Train Epoch: 17 [720/1423 (44%)]\tLoss: 3.371295\n","Train Epoch: 17 [750/1423 (67%)]\tLoss: 1.079520\n","Train Epoch: 17 [1120/1423 (89%)]\tLoss: 0.915586\n","Epoch 17 - Training: Average loss: 1.8871, Accuracy: 68.28%\n","Evaluation set: Average loss: 2.3473, Accuracy: 440/743 (59.22%), F1-score: 0.5740\n","\n","Epsilon Spent: 3.834, Noise_Scale: 1.0\n","Train Epoch: 18 [0/1423 (0%)]\tLoss: 1.084885\n","Train Epoch: 18 [320/1423 (22%)]\tLoss: 1.286948\n","Train Epoch: 18 [680/1423 (44%)]\tLoss: 1.590336\n","Train Epoch: 18 [1080/1423 (67%)]\tLoss: 2.079235\n","Train Epoch: 18 [1200/1423 (89%)]\tLoss: 2.000717\n","Epoch 18 - Training: Average loss: 1.7029, Accuracy: 70.18%\n","Evaluation set: Average loss: 2.3674, Accuracy: 436/743 (58.68%), F1-score: 0.5670\n","\n","Epsilon Spent: 3.945, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 19 [0/1423 (0%)]\tLoss: 1.007610\n","Train Epoch: 19 [370/1423 (22%)]\tLoss: 1.399087\n","Train Epoch: 19 [560/1423 (44%)]\tLoss: 2.368239\n","Train Epoch: 19 [840/1423 (67%)]\tLoss: 1.088928\n","Train Epoch: 19 [920/1423 (89%)]\tLoss: 0.788539\n","Epoch 19 - Training: Average loss: 1.6679, Accuracy: 70.71%\n","Evaluation set: Average loss: 2.4858, Accuracy: 436/743 (58.68%), F1-score: 0.5572\n","\n","Epsilon Spent: 4.055, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 20 [0/1423 (0%)]\tLoss: 1.111449\n","Train Epoch: 20 [430/1423 (22%)]\tLoss: 1.460504\n","Train Epoch: 20 [560/1423 (44%)]\tLoss: 1.653867\n","Train Epoch: 20 [720/1423 (67%)]\tLoss: 1.799511\n","Train Epoch: 20 [1280/1423 (89%)]\tLoss: 0.666835\n","Epoch 20 - Training: Average loss: 1.4779, Accuracy: 71.65%\n","Evaluation set: Average loss: 2.3010, Accuracy: 438/743 (58.95%), F1-score: 0.5634\n","\n","Epsilon Spent: 4.161, Noise_Scale: 1.0\n","Fold 2 is starting ...\n","Validation subjects : ['subject101' 'subject108']\n","Train subjects: ['subject104' 'subject105' 'subject106' 'subject107']\n","Train Epoch: 1 [0/1457 (0%)]\tLoss: 9.332309\n","Train Epoch: 1 [310/1457 (22%)]\tLoss: 6.997649\n","Train Epoch: 1 [560/1457 (43%)]\tLoss: 5.880557\n","Train Epoch: 1 [840/1457 (65%)]\tLoss: 4.399095\n","Train Epoch: 1 [1080/1457 (87%)]\tLoss: 5.183444\n","Epoch 1 - Training: Average loss: 5.7111, Accuracy: 24.01%\n","Evaluation set: Average loss: 4.9912, Accuracy: 209/709 (29.48%), F1-score: 0.2208\n","\n","Epsilon Spent: 1.218, Noise_Scale: 1.0\n","Train Epoch: 2 [0/1457 (0%)]\tLoss: 3.515447\n","Train Epoch: 2 [300/1457 (22%)]\tLoss: 4.575511\n","Train Epoch: 2 [520/1457 (43%)]\tLoss: 3.428788\n","Train Epoch: 2 [900/1457 (65%)]\tLoss: 3.848546\n","Train Epoch: 2 [1040/1457 (87%)]\tLoss: 3.575981\n","Epoch 2 - Training: Average loss: 4.0784, Accuracy: 36.51%\n","Evaluation set: Average loss: 3.6316, Accuracy: 250/709 (35.26%), F1-score: 0.2747\n","\n","Epsilon Spent: 1.514, Noise_Scale: 1.0\n","Train Epoch: 3 [0/1457 (0%)]\tLoss: 2.395722\n","Train Epoch: 3 [270/1457 (22%)]\tLoss: 3.829694\n","Train Epoch: 3 [480/1457 (43%)]\tLoss: 2.850953\n","Train Epoch: 3 [1050/1457 (65%)]\tLoss: 3.556173\n","Train Epoch: 3 [1160/1457 (87%)]\tLoss: 2.720660\n","Epoch 3 - Training: Average loss: 2.9457, Accuracy: 45.00%\n","Evaluation set: Average loss: 3.0165, Accuracy: 290/709 (40.90%), F1-score: 0.3529\n","\n","Epsilon Spent: 1.752, Noise_Scale: 1.0\n","Train Epoch: 4 [0/1457 (0%)]\tLoss: 3.305861\n","Train Epoch: 4 [340/1457 (22%)]\tLoss: 1.601557\n","Train Epoch: 4 [680/1457 (43%)]\tLoss: 3.058190\n","Train Epoch: 4 [900/1457 (65%)]\tLoss: 1.655247\n","Train Epoch: 4 [1400/1457 (87%)]\tLoss: 1.481513\n","Epoch 4 - Training: Average loss: 2.4559, Accuracy: 51.32%\n","Evaluation set: Average loss: 2.7157, Accuracy: 331/709 (46.69%), F1-score: 0.4044\n","\n","Epsilon Spent: 1.96, Noise_Scale: 1.0\n","Train Epoch: 5 [0/1457 (0%)]\tLoss: 2.383534\n","Train Epoch: 5 [180/1457 (22%)]\tLoss: 2.384071\n","Train Epoch: 5 [700/1457 (43%)]\tLoss: 1.850465\n","Train Epoch: 5 [870/1457 (65%)]\tLoss: 3.113286\n","Train Epoch: 5 [960/1457 (87%)]\tLoss: 2.291283\n","Epoch 5 - Training: Average loss: 2.4216, Accuracy: 53.93%\n","Evaluation set: Average loss: 2.6742, Accuracy: 361/709 (50.92%), F1-score: 0.4439\n","\n","Epsilon Spent: 2.148, Noise_Scale: 1.0\n","Train Epoch: 6 [0/1457 (0%)]\tLoss: 1.737017\n","Train Epoch: 6 [310/1457 (22%)]\tLoss: 1.885654\n","Train Epoch: 6 [560/1457 (43%)]\tLoss: 2.482867\n","Train Epoch: 6 [720/1457 (65%)]\tLoss: 2.970864\n","Train Epoch: 6 [1760/1457 (87%)]\tLoss: 1.798869\n","Epoch 6 - Training: Average loss: 2.0645, Accuracy: 61.22%\n","Evaluation set: Average loss: 2.6845, Accuracy: 363/709 (51.20%), F1-score: 0.4380\n","\n","Epsilon Spent: 2.322, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 7 [0/1457 (0%)]\tLoss: 2.036197\n","Train Epoch: 7 [320/1457 (22%)]\tLoss: 3.337127\n","Train Epoch: 7 [660/1457 (43%)]\tLoss: 1.633727\n","Train Epoch: 7 [1050/1457 (65%)]\tLoss: 1.465652\n","Train Epoch: 7 [1560/1457 (87%)]\tLoss: 1.867722\n","Epoch 7 - Training: Average loss: 1.9249, Accuracy: 62.33%\n","Evaluation set: Average loss: 2.3601, Accuracy: 402/709 (56.70%), F1-score: 0.5045\n","\n","Epsilon Spent: 2.485, Noise_Scale: 1.0\n","Train Epoch: 8 [0/1457 (0%)]\tLoss: 3.386206\n","Train Epoch: 8 [240/1457 (22%)]\tLoss: 1.578485\n","Train Epoch: 8 [500/1457 (43%)]\tLoss: 3.615545\n","Train Epoch: 8 [1050/1457 (65%)]\tLoss: 1.993825\n","Train Epoch: 8 [1040/1457 (87%)]\tLoss: 0.968324\n","Epoch 8 - Training: Average loss: 1.8258, Accuracy: 64.96%\n","Evaluation set: Average loss: 2.5023, Accuracy: 401/709 (56.56%), F1-score: 0.5082\n","\n","Epsilon Spent: 2.639, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 9 [0/1457 (0%)]\tLoss: 1.842867\n","Train Epoch: 9 [210/1457 (22%)]\tLoss: 2.553875\n","Train Epoch: 9 [560/1457 (43%)]\tLoss: 0.728686\n","Train Epoch: 9 [780/1457 (65%)]\tLoss: 2.661252\n","Train Epoch: 9 [1160/1457 (87%)]\tLoss: 1.276532\n","Epoch 9 - Training: Average loss: 1.6605, Accuracy: 66.55%\n","Evaluation set: Average loss: 2.4302, Accuracy: 412/709 (58.11%), F1-score: 0.5293\n","\n","Epsilon Spent: 2.786, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 10 [0/1457 (0%)]\tLoss: 1.028910\n","Train Epoch: 10 [300/1457 (22%)]\tLoss: 2.238983\n","Train Epoch: 10 [760/1457 (43%)]\tLoss: 2.552366\n","Train Epoch: 10 [1050/1457 (65%)]\tLoss: 1.639751\n","Train Epoch: 10 [1080/1457 (87%)]\tLoss: 2.114895\n","Epoch 10 - Training: Average loss: 1.6740, Accuracy: 68.57%\n","Evaluation set: Average loss: 2.3026, Accuracy: 438/709 (61.78%), F1-score: 0.5780\n","\n","Epsilon Spent: 2.926, Noise_Scale: 1.0\n","Train Epoch: 11 [0/1457 (0%)]\tLoss: 3.099433\n","Train Epoch: 11 [290/1457 (22%)]\tLoss: 1.282007\n","Train Epoch: 11 [560/1457 (43%)]\tLoss: 1.445000\n","Train Epoch: 11 [810/1457 (65%)]\tLoss: 1.070784\n","Train Epoch: 11 [1400/1457 (87%)]\tLoss: 3.111214\n","Epoch 11 - Training: Average loss: 1.5780, Accuracy: 70.73%\n","Evaluation set: Average loss: 2.2924, Accuracy: 427/709 (60.23%), F1-score: 0.5601\n","\n","Epsilon Spent: 3.06, Noise_Scale: 1.0\n","Train Epoch: 12 [0/1457 (0%)]\tLoss: 2.036810\n","Train Epoch: 12 [390/1457 (22%)]\tLoss: 0.845361\n","Train Epoch: 12 [700/1457 (43%)]\tLoss: 2.750141\n","Train Epoch: 12 [810/1457 (65%)]\tLoss: 2.506742\n","Train Epoch: 12 [1040/1457 (87%)]\tLoss: 0.573312\n","Epoch 12 - Training: Average loss: 1.7489, Accuracy: 68.58%\n","Evaluation set: Average loss: 2.2836, Accuracy: 444/709 (62.62%), F1-score: 0.5796\n","\n","Epsilon Spent: 3.19, Noise_Scale: 1.0\n","Train Epoch: 13 [0/1457 (0%)]\tLoss: 1.128413\n","Train Epoch: 13 [360/1457 (22%)]\tLoss: 2.163833\n","Train Epoch: 13 [800/1457 (43%)]\tLoss: 1.392726\n","Train Epoch: 13 [930/1457 (65%)]\tLoss: 1.142412\n","Train Epoch: 13 [2000/1457 (87%)]\tLoss: 1.144567\n","Epoch 13 - Training: Average loss: 1.4112, Accuracy: 71.98%\n","Evaluation set: Average loss: 2.3210, Accuracy: 440/709 (62.06%), F1-score: 0.5792\n","\n","Epsilon Spent: 3.316, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 14 [0/1457 (0%)]\tLoss: 1.039214\n","Train Epoch: 14 [320/1457 (22%)]\tLoss: 0.753169\n","Train Epoch: 14 [720/1457 (43%)]\tLoss: 0.797203\n","Train Epoch: 14 [870/1457 (65%)]\tLoss: 0.934829\n","Train Epoch: 14 [1200/1457 (87%)]\tLoss: 2.051141\n","Epoch 14 - Training: Average loss: 1.5230, Accuracy: 70.52%\n","Evaluation set: Average loss: 2.2710, Accuracy: 438/709 (61.78%), F1-score: 0.5696\n","\n","Epsilon Spent: 3.438, Noise_Scale: 1.0\n","Train Epoch: 15 [0/1457 (0%)]\tLoss: 2.141462\n","Train Epoch: 15 [240/1457 (22%)]\tLoss: 2.329405\n","Train Epoch: 15 [760/1457 (43%)]\tLoss: 2.175940\n","Train Epoch: 15 [930/1457 (65%)]\tLoss: 1.336798\n","Train Epoch: 15 [1280/1457 (87%)]\tLoss: 0.662317\n","Epoch 15 - Training: Average loss: 1.3471, Accuracy: 73.16%\n","Evaluation set: Average loss: 2.4573, Accuracy: 441/709 (62.20%), F1-score: 0.5801\n","\n","Epsilon Spent: 3.556, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 16 [0/1457 (0%)]\tLoss: 1.307987\n","Train Epoch: 16 [220/1457 (22%)]\tLoss: 1.895247\n","Train Epoch: 16 [540/1457 (43%)]\tLoss: 1.640381\n","Train Epoch: 16 [1050/1457 (65%)]\tLoss: 1.571561\n","Train Epoch: 16 [1480/1457 (87%)]\tLoss: 1.930145\n","Epoch 16 - Training: Average loss: 1.3906, Accuracy: 75.17%\n","Evaluation set: Average loss: 2.3188, Accuracy: 452/709 (63.75%), F1-score: 0.6018\n","\n","Epsilon Spent: 3.672, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 17 [0/1457 (0%)]\tLoss: 1.969203\n","Train Epoch: 17 [390/1457 (22%)]\tLoss: 1.291898\n","Train Epoch: 17 [760/1457 (43%)]\tLoss: 1.905233\n","Train Epoch: 17 [930/1457 (65%)]\tLoss: 1.711652\n","Train Epoch: 17 [1280/1457 (87%)]\tLoss: 0.864972\n","Epoch 17 - Training: Average loss: 1.4818, Accuracy: 72.11%\n","Evaluation set: Average loss: 2.4332, Accuracy: 432/709 (60.93%), F1-score: 0.5788\n","\n","Epsilon Spent: 3.784, Noise_Scale: 1.0\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 18 [0/1457 (0%)]\tLoss: 2.037115\n","Train Epoch: 18 [300/1457 (22%)]\tLoss: 1.511294\n","Train Epoch: 18 [500/1457 (43%)]\tLoss: 2.044599\n","Train Epoch: 18 [750/1457 (65%)]\tLoss: 1.167456\n","Train Epoch: 18 [1080/1457 (87%)]\tLoss: 2.339803\n","Epoch 18 - Training: Average loss: 1.4198, Accuracy: 72.90%\n","Evaluation set: Average loss: 2.4489, Accuracy: 448/709 (63.19%), F1-score: 0.5952\n","\n","Epsilon Spent: 3.894, Noise_Scale: 1.0\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 19 [0/1457 (0%)]\tLoss: 1.106084\n","Train Epoch: 19 [330/1457 (22%)]\tLoss: 1.448279\n","Train Epoch: 19 [620/1457 (43%)]\tLoss: 2.890999\n","Train Epoch: 19 [1200/1457 (65%)]\tLoss: 1.123454\n","Train Epoch: 19 [1360/1457 (87%)]\tLoss: 0.872380\n","Epoch 19 - Training: Average loss: 1.3035, Accuracy: 74.97%\n","Evaluation set: Average loss: 2.3239, Accuracy: 455/709 (64.17%), F1-score: 0.6051\n","\n","Epsilon Spent: 4.002, Noise_Scale: 1.0\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 20 [0/1457 (0%)]\tLoss: 1.577133\n","Train Epoch: 20 [310/1457 (22%)]\tLoss: 1.231364\n","Train Epoch: 20 [560/1457 (43%)]\tLoss: 1.024418\n","Train Epoch: 20 [1140/1457 (65%)]\tLoss: 1.152609\n","Train Epoch: 20 [1360/1457 (87%)]\tLoss: 1.335094\n","Epoch 20 - Training: Average loss: 1.4489, Accuracy: 73.93%\n","Evaluation set: Average loss: 2.4336, Accuracy: 442/709 (62.34%), F1-score: 0.5938\n","\n","Epsilon Spent: 4.107, Noise_Scale: 1.0\n","EarlyStopping counter: 6 out of 10\n","Fold 3 is starting ...\n","Validation subjects : ['subject106' 'subject107']\n","Train subjects: ['subject101' 'subject104' 'subject105' 'subject108']\n","Train Epoch: 1 [0/1452 (0%)]\tLoss: 8.995121\n","Train Epoch: 1 [250/1452 (22%)]\tLoss: 6.859480\n","Train Epoch: 1 [640/1452 (43%)]\tLoss: 7.569298\n","Train Epoch: 1 [870/1452 (65%)]\tLoss: 4.929409\n","Train Epoch: 1 [1760/1452 (87%)]\tLoss: 6.189166\n","Epoch 1 - Training: Average loss: 5.7163, Accuracy: 24.49%\n","Evaluation set: Average loss: 5.1689, Accuracy: 226/714 (31.65%), F1-score: 0.2556\n","\n","Epsilon Spent: 1.218, Noise_Scale: 1.0\n","Train Epoch: 2 [0/1452 (0%)]\tLoss: 4.748591\n","Train Epoch: 2 [290/1452 (22%)]\tLoss: 3.876949\n","Train Epoch: 2 [560/1452 (43%)]\tLoss: 4.089345\n","Train Epoch: 2 [810/1452 (65%)]\tLoss: 4.433041\n","Train Epoch: 2 [1200/1452 (87%)]\tLoss: 3.543688\n","Epoch 2 - Training: Average loss: 3.7296, Accuracy: 38.60%\n","Evaluation set: Average loss: 3.6458, Accuracy: 294/714 (41.18%), F1-score: 0.3481\n","\n","Epsilon Spent: 1.514, Noise_Scale: 1.0\n","Train Epoch: 3 [0/1452 (0%)]\tLoss: 3.386995\n","Train Epoch: 3 [370/1452 (22%)]\tLoss: 2.658719\n","Train Epoch: 3 [560/1452 (43%)]\tLoss: 2.641203\n","Train Epoch: 3 [900/1452 (65%)]\tLoss: 2.856022\n","Train Epoch: 3 [1160/1452 (87%)]\tLoss: 3.083181\n","Epoch 3 - Training: Average loss: 2.9351, Accuracy: 42.06%\n","Evaluation set: Average loss: 2.7541, Accuracy: 343/714 (48.04%), F1-score: 0.4328\n","\n","Epsilon Spent: 1.752, Noise_Scale: 1.0\n","Train Epoch: 4 [0/1452 (0%)]\tLoss: 3.089199\n","Train Epoch: 4 [350/1452 (22%)]\tLoss: 2.583605\n","Train Epoch: 4 [720/1452 (43%)]\tLoss: 2.124739\n","Train Epoch: 4 [1020/1452 (65%)]\tLoss: 2.063423\n","Train Epoch: 4 [1280/1452 (87%)]\tLoss: 2.236812\n","Epoch 4 - Training: Average loss: 2.3718, Accuracy: 51.52%\n","Evaluation set: Average loss: 2.5680, Accuracy: 389/714 (54.48%), F1-score: 0.4940\n","\n","Epsilon Spent: 1.96, Noise_Scale: 1.0\n","Train Epoch: 5 [0/1452 (0%)]\tLoss: 2.623199\n","Train Epoch: 5 [270/1452 (22%)]\tLoss: 2.532898\n","Train Epoch: 5 [780/1452 (43%)]\tLoss: 2.380446\n","Train Epoch: 5 [990/1452 (65%)]\tLoss: 2.402001\n","Train Epoch: 5 [1360/1452 (87%)]\tLoss: 1.887537\n","Epoch 5 - Training: Average loss: 2.1822, Accuracy: 54.17%\n","Evaluation set: Average loss: 2.2923, Accuracy: 433/714 (60.64%), F1-score: 0.5562\n","\n","Epsilon Spent: 2.148, Noise_Scale: 1.0\n","Train Epoch: 6 [0/1452 (0%)]\tLoss: 2.127217\n","Train Epoch: 6 [260/1452 (22%)]\tLoss: 2.689084\n","Train Epoch: 6 [600/1452 (43%)]\tLoss: 2.260357\n","Train Epoch: 6 [900/1452 (65%)]\tLoss: 3.031141\n","Train Epoch: 6 [1160/1452 (87%)]\tLoss: 2.714708\n","Epoch 6 - Training: Average loss: 2.2017, Accuracy: 57.51%\n","Evaluation set: Average loss: 2.2199, Accuracy: 449/714 (62.89%), F1-score: 0.5721\n","\n","Epsilon Spent: 2.322, Noise_Scale: 1.0\n","Train Epoch: 7 [0/1452 (0%)]\tLoss: 1.958590\n","Train Epoch: 7 [270/1452 (22%)]\tLoss: 2.171161\n","Train Epoch: 7 [440/1452 (43%)]\tLoss: 2.044912\n","Train Epoch: 7 [1080/1452 (65%)]\tLoss: 2.310667\n","Train Epoch: 7 [1240/1452 (87%)]\tLoss: 0.901357\n","Epoch 7 - Training: Average loss: 1.9472, Accuracy: 60.56%\n","Evaluation set: Average loss: 2.0937, Accuracy: 454/714 (63.59%), F1-score: 0.5851\n","\n","Epsilon Spent: 2.485, Noise_Scale: 1.0\n","Train Epoch: 8 [0/1452 (0%)]\tLoss: 0.854890\n","Train Epoch: 8 [250/1452 (22%)]\tLoss: 2.558550\n","Train Epoch: 8 [480/1452 (43%)]\tLoss: 1.259285\n","Train Epoch: 8 [990/1452 (65%)]\tLoss: 2.064186\n","Train Epoch: 8 [1000/1452 (87%)]\tLoss: 1.412472\n","Epoch 8 - Training: Average loss: 1.6778, Accuracy: 64.70%\n","Evaluation set: Average loss: 1.9071, Accuracy: 476/714 (66.67%), F1-score: 0.6142\n","\n","Epsilon Spent: 2.639, Noise_Scale: 1.0\n","Train Epoch: 9 [0/1452 (0%)]\tLoss: 1.922028\n","Train Epoch: 9 [330/1452 (22%)]\tLoss: 0.936577\n","Train Epoch: 9 [780/1452 (43%)]\tLoss: 1.883878\n","Train Epoch: 9 [720/1452 (65%)]\tLoss: 2.181060\n","Train Epoch: 9 [1320/1452 (87%)]\tLoss: 1.262187\n","Epoch 9 - Training: Average loss: 1.6291, Accuracy: 64.68%\n","Evaluation set: Average loss: 1.7676, Accuracy: 488/714 (68.35%), F1-score: 0.6316\n","\n","Epsilon Spent: 2.786, Noise_Scale: 1.0\n","Train Epoch: 10 [0/1452 (0%)]\tLoss: 0.843960\n","Train Epoch: 10 [240/1452 (22%)]\tLoss: 0.981051\n","Train Epoch: 10 [560/1452 (43%)]\tLoss: 1.047491\n","Train Epoch: 10 [1050/1452 (65%)]\tLoss: 2.245308\n","Train Epoch: 10 [1040/1452 (87%)]\tLoss: 1.313632\n","Epoch 10 - Training: Average loss: 1.5285, Accuracy: 67.56%\n","Evaluation set: Average loss: 1.7765, Accuracy: 504/714 (70.59%), F1-score: 0.6558\n","\n","Epsilon Spent: 2.926, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 11 [0/1452 (0%)]\tLoss: 1.194917\n","Train Epoch: 11 [230/1452 (22%)]\tLoss: 1.708874\n","Train Epoch: 11 [540/1452 (43%)]\tLoss: 2.173794\n","Train Epoch: 11 [900/1452 (65%)]\tLoss: 1.279842\n","Train Epoch: 11 [960/1452 (87%)]\tLoss: 1.051979\n","Epoch 11 - Training: Average loss: 1.5988, Accuracy: 68.63%\n","Evaluation set: Average loss: 1.7259, Accuracy: 517/714 (72.41%), F1-score: 0.6750\n","\n","Epsilon Spent: 3.06, Noise_Scale: 1.0\n","Train Epoch: 12 [0/1452 (0%)]\tLoss: 0.777287\n","Train Epoch: 12 [370/1452 (22%)]\tLoss: 0.796738\n","Train Epoch: 12 [680/1452 (43%)]\tLoss: 0.811046\n","Train Epoch: 12 [780/1452 (65%)]\tLoss: 2.703474\n","Train Epoch: 12 [1400/1452 (87%)]\tLoss: 2.302001\n","Epoch 12 - Training: Average loss: 1.6110, Accuracy: 68.96%\n","Evaluation set: Average loss: 1.7166, Accuracy: 508/714 (71.15%), F1-score: 0.6585\n","\n","Epsilon Spent: 3.19, Noise_Scale: 1.0\n","Train Epoch: 13 [0/1452 (0%)]\tLoss: 1.992658\n","Train Epoch: 13 [310/1452 (22%)]\tLoss: 2.085719\n","Train Epoch: 13 [660/1452 (43%)]\tLoss: 1.673663\n","Train Epoch: 13 [780/1452 (65%)]\tLoss: 1.659525\n","Train Epoch: 13 [1240/1452 (87%)]\tLoss: 1.808804\n","Epoch 13 - Training: Average loss: 1.6025, Accuracy: 68.27%\n","Evaluation set: Average loss: 1.6260, Accuracy: 492/714 (68.91%), F1-score: 0.6390\n","\n","Epsilon Spent: 3.316, Noise_Scale: 1.0\n","Train Epoch: 14 [0/1452 (0%)]\tLoss: 1.085308\n","Train Epoch: 14 [260/1452 (22%)]\tLoss: 0.766149\n","Train Epoch: 14 [880/1452 (43%)]\tLoss: 1.331829\n","Train Epoch: 14 [720/1452 (65%)]\tLoss: 1.706699\n","Train Epoch: 14 [1080/1452 (87%)]\tLoss: 2.406841\n","Epoch 14 - Training: Average loss: 1.5676, Accuracy: 69.30%\n","Evaluation set: Average loss: 1.5681, Accuracy: 509/714 (71.29%), F1-score: 0.6592\n","\n","Epsilon Spent: 3.438, Noise_Scale: 1.0\n","Train Epoch: 15 [0/1452 (0%)]\tLoss: 2.166302\n","Train Epoch: 15 [300/1452 (22%)]\tLoss: 0.916933\n","Train Epoch: 15 [520/1452 (43%)]\tLoss: 0.730929\n","Train Epoch: 15 [990/1452 (65%)]\tLoss: 1.003489\n","Train Epoch: 15 [1000/1452 (87%)]\tLoss: 0.837590\n","Epoch 15 - Training: Average loss: 1.6604, Accuracy: 69.85%\n","Evaluation set: Average loss: 1.6286, Accuracy: 505/714 (70.73%), F1-score: 0.6630\n","\n","Epsilon Spent: 3.556, Noise_Scale: 1.0\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 16 [0/1452 (0%)]\tLoss: 0.676404\n","Train Epoch: 16 [350/1452 (22%)]\tLoss: 1.120145\n","Train Epoch: 16 [900/1452 (43%)]\tLoss: 1.577132\n","Train Epoch: 16 [990/1452 (65%)]\tLoss: 0.958917\n","Train Epoch: 16 [1160/1452 (87%)]\tLoss: 1.090265\n","Epoch 16 - Training: Average loss: 1.2597, Accuracy: 73.18%\n","Evaluation set: Average loss: 1.5745, Accuracy: 524/714 (73.39%), F1-score: 0.6830\n","\n","Epsilon Spent: 3.672, Noise_Scale: 1.0\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 17 [0/1452 (0%)]\tLoss: 1.178587\n","Train Epoch: 17 [280/1452 (22%)]\tLoss: 0.786158\n","Train Epoch: 17 [660/1452 (43%)]\tLoss: 2.578772\n","Train Epoch: 17 [990/1452 (65%)]\tLoss: 0.500075\n","Train Epoch: 17 [1600/1452 (87%)]\tLoss: 1.343062\n","Epoch 17 - Training: Average loss: 1.4144, Accuracy: 72.39%\n","Evaluation set: Average loss: 1.6504, Accuracy: 519/714 (72.69%), F1-score: 0.6791\n","\n","Epsilon Spent: 3.784, Noise_Scale: 1.0\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 18 [0/1452 (0%)]\tLoss: 1.070321\n","Train Epoch: 18 [310/1452 (22%)]\tLoss: 0.909250\n","Train Epoch: 18 [700/1452 (43%)]\tLoss: 2.037357\n","Train Epoch: 18 [720/1452 (65%)]\tLoss: 1.306406\n","Train Epoch: 18 [1080/1452 (87%)]\tLoss: 1.305200\n","Epoch 18 - Training: Average loss: 1.4757, Accuracy: 72.40%\n","Evaluation set: Average loss: 1.6124, Accuracy: 512/714 (71.71%), F1-score: 0.6610\n","\n","Epsilon Spent: 3.894, Noise_Scale: 1.0\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 19 [0/1452 (0%)]\tLoss: 1.761368\n","Train Epoch: 19 [400/1452 (22%)]\tLoss: 1.161789\n","Train Epoch: 19 [620/1452 (43%)]\tLoss: 1.169557\n","Train Epoch: 19 [900/1452 (65%)]\tLoss: 1.547687\n","Train Epoch: 19 [1160/1452 (87%)]\tLoss: 0.889485\n","Epoch 19 - Training: Average loss: 1.3970, Accuracy: 72.44%\n","Evaluation set: Average loss: 1.7131, Accuracy: 523/714 (73.25%), F1-score: 0.6816\n","\n","Epsilon Spent: 4.002, Noise_Scale: 1.0\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 20 [0/1452 (0%)]\tLoss: 1.478841\n","Train Epoch: 20 [340/1452 (22%)]\tLoss: 0.990543\n","Train Epoch: 20 [600/1452 (43%)]\tLoss: 1.223582\n","Train Epoch: 20 [1110/1452 (65%)]\tLoss: 1.692744\n","Train Epoch: 20 [1360/1452 (87%)]\tLoss: 1.369043\n","Epoch 20 - Training: Average loss: 1.4160, Accuracy: 73.62%\n","Evaluation set: Average loss: 1.7217, Accuracy: 514/714 (71.99%), F1-score: 0.6720\n","\n","Epsilon Spent: 4.107, Noise_Scale: 1.0\n","EarlyStopping counter: 6 out of 10\n"]}]},{"cell_type":"markdown","source":["## EVALUATION ON TEST DATA (Subject 2 & Subject 3)"],"metadata":{"id":"r5MKnHYdbcv4"}},{"cell_type":"code","source":["import torch\n","import gc\n","import os\n","\n","# Enable detailed CUDA error reporting for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","# Clear CUDA cache first\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","final_model_path = f'attack_results_DP/final_models/{final_model_version}'\n","final_scaler_path = f'attack_results_DP/final_models/{final_scaler_version}'\n","final_label_encoder_path = f'attack_results_DP/final_models/{final_label_encoder_version}'\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","try:\n","    original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')\n","    final_test_model = copy.deepcopy(original_model)\n","\n","    # CRITICAL FIX: Load to CPU first, then move to device\n","    print(\"Loading model state dict...\")\n","    state_dict = torch.load(final_model_path, weights_only=True, map_location='cpu')\n","    print(\"Applying state dict...\")\n","    final_test_model.load_state_dict(remove_module_prefix(state_dict))\n","    print(\"Moving model to device...\")\n","    final_test_model.to(device)\n","\n","    print(\"Loading scaler and label encoder...\")\n","    final_scaler_test = pickle.load(open(final_scaler_path, 'rb'))\n","    final_le_test = pickle.load(open(final_label_encoder_path, 'rb'))\n","\n","    print(\"Preparing test data...\")\n","    X_test_mia = np.array(mia_test_X)\n","    y_test_mia = np.array(mia_test_y)\n","\n","    encoded_y_test_mia = final_le_test.transform(y_test_mia)\n","\n","    # Debug info\n","    print(f\"Original test labels: {np.unique(y_test_mia)}\")\n","    print(f\"Encoded test labels: {np.unique(encoded_y_test_mia)}\")\n","    print(f\"Label range: {np.min(encoded_y_test_mia)} to {np.max(encoded_y_test_mia)}\")\n","    print(f\"N_CLASSES: {N_CLASSES}\")\n","\n","    scaled_X_test_mia = final_scaler_test.transform(X_test_mia)\n","    mia_test_data_loader = get_data_loader(scaled_X_test_mia, encoded_y_test_mia, BATCH_SIZE=BATCH_SIZE_LIST[0], shuffle=False)\n","\n","    print(\"Running evaluation...\")\n","    final_test_loss, final_test_acc, final_test_f1 = evaluate(final_test_model, mia_test_data_loader, criterion, device)\n","\n","    print(f'Final Model Test Loss: {final_test_loss}, Test Acc: {final_test_acc}, Test F1: {final_test_f1}')\n","\n","except RuntimeError as e:\n","    print(f\"CUDA Error occurred: {e}\")\n","    print(\"Attempting CPU fallback...\")\n","\n","    # Fallback to CPU\n","    device_cpu = torch.device('cpu')\n","\n","    original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')\n","    final_test_model = copy.deepcopy(original_model)\n","\n","    state_dict = torch.load(final_model_path, weights_only=True, map_location='cpu')\n","    final_test_model.load_state_dict(remove_module_prefix(state_dict))\n","    final_test_model.to(device_cpu)  # Use CPU\n","\n","    final_scaler_test = pickle.load(open(final_scaler_path, 'rb'))\n","    final_le_test = pickle.load(open(final_label_encoder_path, 'rb'))\n","\n","    X_test_mia = np.array(mia_test_X)\n","    y_test_mia = np.array(mia_test_y)\n","    encoded_y_test_mia = final_le_test.transform(y_test_mia)\n","    scaled_X_test_mia = final_scaler_test.transform(X_test_mia)\n","\n","    mia_test_data_loader = get_data_loader(scaled_X_test_mia, encoded_y_test_mia, BATCH_SIZE=BATCH_SIZE_LIST[0], shuffle=False)\n","\n","    final_test_loss, final_test_acc, final_test_f1 = evaluate(final_test_model, mia_test_data_loader, criterion, device_cpu)\n","\n","    print(f'Final Model Test Loss (CPU): {final_test_loss}, Test Acc: {final_test_acc}, Test F1: {final_test_f1}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgQ8Rwq3XtP7","executionInfo":{"status":"ok","timestamp":1750894674433,"user_tz":-120,"elapsed":10667,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"491ea228-06cc-4a4b-abb7-27a37e668ecb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/OxWearables/ssl-wearables/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"stream","name":"stdout","text":["131 Weights loaded\n","Loading model state dict...\n","Applying state dict...\n","Moving model to device...\n","Loading scaler and label encoder...\n","Preparing test data...\n","Original test labels: [ 1  2  3  4 12 13 16 17]\n","Encoded test labels: [0 1 2 3 4 5 6 7]\n","Label range: 0 to 7\n","N_CLASSES: 8\n","Running evaluation...\n","Evaluation set: Average loss: 2.2889, Accuracy: 427/721 (59.22%), F1-score: 0.5614\n","\n","Final Model Test Loss: 2.2888554999715907, Test Acc: 59.22330097087379, Test F1: 0.5614112570604692\n"]}]},{"cell_type":"markdown","source":["## FINE TUNING of CLASSIFIER HEADS (no DP)"],"metadata":{"id":"Ik9yfLZBVj6I"}},{"cell_type":"code","source":["\n","import datetime as dt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedGroupKFold, LeaveOneGroupOut\n","import torch.nn as nn\n","import copy\n","from itertools import product\n","\n","\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","reports_save = True\n","\n","label_encoder = LabelEncoder()\n","\n","X_combined = []\n","y_combined = []\n","subject_groups_combined = []\n","\n","mia_test_X = []\n","mia_test_y = []\n","test_groups_combined = []\n","mia_test_subjects = []\n","\n","for sb_name, Xs, ys in all_subject_infos:\n","  if sb_name not in ['subject102','subject103']:           # Test subject for membership inference attack as a non-member\n","    X_combined.extend(Xs)\n","    y_combined.extend(ys)\n","    subject_groups_combined.extend([sb_name] * len(ys))\n","  else:\n","    mia_test_subjects.append(sb_name)\n","    mia_test_X.extend(Xs)\n","    mia_test_y.extend(ys)\n","    test_groups_combined.extend([sb_name] * len(ys))\n","\n","\n","\n","X_combined = np.array(X_combined)\n","y_combined = np.array(y_combined)\n","subject_groups_combined = np.array(subject_groups_combined)\n","\n","label_encoder.fit(y_combined)\n","encoded_y_combined_raw = label_encoder.transform(y_combined)\n","\n","EPOCHS = 20\n","BATCH_SIZE_LIST = [32]\n","LRATE_LIST = [1e-3]\n","\n","\n","parameter_pairs = list(product(BATCH_SIZE_LIST, LRATE_LIST))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","N_CLASSES = len(label_encoder.classes_)\n","g_noise_multp = 1.0\n","C_value = 1.0\n","noise_scale = g_noise_multp * C_value\n","DELTA = 1e-5\n","\n","original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yat_-GICVjQQ","executionInfo":{"status":"ok","timestamp":1750894788859,"user_tz":-120,"elapsed":7829,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"b400d151-bd32-4f9f-fb39-3ed08f5a80cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/OxWearables/ssl-wearables/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"stream","name":"stdout","text":["131 Weights loaded\n"]}]},{"cell_type":"code","source":["final_best_model = None\n","final_best_val_loss = float('inf')\n","final_best_val_acc = 0\n","final_best_val_f1 = 0\n","final_scaler = None\n","final_best_epoch = None\n","final_best_train_acc = None\n","final_best_train_loss = None\n","final_best_train_subjects = None\n","final_best_val_subjects = None\n","final_epsilon_spent = None\n","final_BATCH_SIZE = None\n","final_LRATE = None\n","\n","for BATCH_SIZE, LRATE in parameter_pairs:\n","  print('-------------------------------------------------------------------------------------------------')\n","  print(f'Batch size: {BATCH_SIZE}, Learning rate: {LRATE}')\n","  print('-------------------------------------------------------------------------------------------------')\n","\n","  fold_details = {\"Val Subjects\": [], \"Batch_Size\": BATCH_SIZE, \"Learning_Rate\": LRATE, \"Epoch_Results\": []}\n","\n","  # Leave One Group Out for inner fold\n","  inner_skf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n","  for fold, (train_index, val_index) in enumerate(inner_skf.split(X_combined, encoded_y_combined_raw, subject_groups_combined), start=1):\n","    print(f'Fold {fold} is starting ...')\n","    X_train, X_val = X_combined[train_index], X_combined[val_index]\n","    y_train, y_val = encoded_y_combined_raw[train_index], encoded_y_combined_raw[val_index]\n","    val_subjects = np.unique(subject_groups_combined[val_index])\n","    train_subjects = np.unique(subject_groups_combined[train_index])\n","    print(f'Validation subjects : {val_subjects}')\n","    print(f'Train subjects: {train_subjects}')\n","\n","\n","    scaler = CustomScaler()\n","    scaled_X_train = scaler.fit_transform(X_train)\n","    scaled_X_val = scaler.transform(X_val)\n","    train_data_loader = get_data_loader(scaled_X_train, y_train, BATCH_SIZE=BATCH_SIZE, shuffle=True)\n","    val_data_loader = get_data_loader(scaled_X_val, y_val, BATCH_SIZE=BATCH_SIZE, shuffle=False)\n","    model = copy.deepcopy(original_model)\n","    model.to(device)\n","    for name, param in model.named_parameters():\n","      if name.startswith('classifier'):\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = False\n","    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LRATE, weight_decay=0.01)\n","    criterion = nn.CrossEntropyLoss()\n","    early_stopping = EarlyStopping(patience=10, verbose=True)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n","\n","    val_losses = []\n","    val_accuracies = []\n","    val_f1s = []\n","    train_losses = []\n","    train_accuracies = []\n","    epochs_list = []\n","    epoch_results = {\"epoch\": [], \"val_subjects\":'-'.join(val_subjects), \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","    best_scaler = None\n","    best_epoch = None\n","    for epoch in range(1, EPOCHS + 1):\n","      train_loss, train_acc = train(model, train_data_loader, optimizer, criterion, epoch, device)\n","      val_loss, val_acc, val_f1 = evaluate(model, val_data_loader, criterion, device)\n","      scheduler.step(val_loss)\n","      train_losses.append(train_loss)\n","      train_accuracies.append(train_acc)\n","      val_losses.append(val_loss)\n","      val_accuracies.append(val_acc)\n","      val_f1s.append(val_f1)\n","      epochs_list.append(epoch)\n","\n","      epoch_results[\"epoch\"].append(epoch)\n","      epoch_results[\"train_loss\"].append(train_loss)\n","      epoch_results[\"train_acc\"].append(train_acc)\n","      epoch_results[\"val_loss\"].append(val_loss)\n","      epoch_results[\"val_acc\"].append(val_acc)\n","      epoch_results[\"val_f1\"].append(val_f1)\n","      if val_loss < final_best_val_loss:\n","        final_best_val_loss = val_loss\n","        final_best_model = copy.deepcopy(model.state_dict())\n","        final_best_val_acc = val_acc\n","        final_best_val_f1 = val_f1\n","        final_scaler = copy.deepcopy(scaler)\n","        final_best_epoch = epoch\n","        final_best_train_acc = train_acc\n","        final_best_train_loss = train_loss\n","        final_best_train_subjects = train_subjects\n","        final_best_val_subjects = val_subjects\n","        final_BATCH_SIZE = BATCH_SIZE\n","        final_LRATE = LRATE\n","\n","\n","      early_stopping(val_loss, model, epoch)\n","      if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n","\n","    fold_details[\"Val Subjects\"].append(val_subjects)\n","    fold_details[\"Epoch_Results\"].append(epoch_results)\n","\n","\n","today_time = dt.date.today().strftime(\"%Y%m%d\")\n","vers = f'{window_length_sec}_{sampling_rate_hz}_{overlap_ratio}_{today_time}'\n","\n","\n","fold_details_df = pd.DataFrame(fold_details)\n","\n","noise_scale_vers = int(noise_scale)\n","\n","final_model_version = f'final_model_{vers}.pth'\n","final_scaler_version = f'final_scaler_{vers}.pkl'\n","final_model_df = pd.DataFrame({\"Final_Model_Version\":[final_model_version],\n","                               \"Final_Scaler_Version\":[final_scaler_version],\n","                               \"Final_Best_Val_Loss\":final_best_val_loss,\n","                               \"Final_Best_Val_Acc\":final_best_val_acc,\n","                               \"Final_Best_Val_F1\":final_best_val_f1,\n","                               \"Final_Best_Epoch\":final_best_epoch,\n","                               \"Final_Best_Train_Acc\":final_best_train_acc,\n","                               \"Final_Best_Train_Loss\":final_best_train_loss,\n","                               \"Final_Best_Train_Subjects\":'-'.join(final_best_train_subjects),\n","                               \"Final_Best_Val_Subjects\":'-'.join(final_best_val_subjects),\n","                               \"Final_Epsilon_Spent\":final_epsilon_spent,\n","                               \"Final_Batch_Size\":final_BATCH_SIZE,\n","                               \"Final_Learning_Rate\":final_LRATE,\n","                               })\n","\n","\n","\n","if reports_save:\n","  directory_name = 'attack_results_noDP'\n","  os.makedirs(directory_name, exist_ok=True)\n","\n","  model_directory_name = 'attack_results_noDP/final_models'\n","  os.makedirs(model_directory_name, exist_ok=True)\n","\n","\n","  torch.save(final_best_model, os.path.join(model_directory_name, final_model_version))\n","\n","\n","  with open(os.path.join(model_directory_name, final_scaler_version), 'wb') as f:\n","    pickle.dump(final_scaler, f)\n","\n","\n","  final_label_encoder_version = f'final_label_encoder_{vers}.pkl'\n","  with open(os.path.join(model_directory_name, final_label_encoder_version), 'wb') as f:\n","    pickle.dump(label_encoder, f)\n","\n","  fold_details_df.to_csv(f'{directory_name}/fold_details_noDP_CH_NS_{vers}_Split_4-2-2.csv')\n","  final_model_df.to_csv(f'{directory_name}/final_model_df_noDP_CH_NS_{vers}_Split_4-2-2.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZxJ3LGZV3sA","executionInfo":{"status":"ok","timestamp":1750894853119,"user_tz":-120,"elapsed":35911,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"cfafd0d7-17c1-4e33-9634-36bc584c3721"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------------------------------------------------\n","Batch size: 32, Learning rate: 0.001\n","-------------------------------------------------------------------------------------------------\n","Fold 1 is starting ...\n","Validation subjects : ['subject104' 'subject105']\n","Train subjects: ['subject101' 'subject106' 'subject107' 'subject108']\n","Train Epoch: 1 [0/1423 (0%)]\tLoss: 6.828506\n","Train Epoch: 1 [320/1423 (22%)]\tLoss: 3.129222\n","Train Epoch: 1 [640/1423 (44%)]\tLoss: 2.524057\n","Train Epoch: 1 [960/1423 (67%)]\tLoss: 1.817114\n","Train Epoch: 1 [1280/1423 (89%)]\tLoss: 2.530724\n","Epoch 1 - Training: Average loss: 2.8115, Accuracy: 64.65%\n","Evaluation set: Average loss: 2.6047, Accuracy: 509/743 (68.51%), F1-score: 0.6592\n","\n","Train Epoch: 2 [0/1423 (0%)]\tLoss: 1.020925\n","Train Epoch: 2 [320/1423 (22%)]\tLoss: 2.046731\n","Train Epoch: 2 [640/1423 (44%)]\tLoss: 1.770709\n","Train Epoch: 2 [960/1423 (67%)]\tLoss: 0.839329\n","Train Epoch: 2 [1280/1423 (89%)]\tLoss: 1.317361\n","Epoch 2 - Training: Average loss: 1.3272, Accuracy: 78.71%\n","Evaluation set: Average loss: 2.8467, Accuracy: 444/743 (59.76%), F1-score: 0.5941\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 3 [0/1423 (0%)]\tLoss: 0.319185\n","Train Epoch: 3 [320/1423 (22%)]\tLoss: 0.199123\n","Train Epoch: 3 [640/1423 (44%)]\tLoss: 1.090819\n","Train Epoch: 3 [960/1423 (67%)]\tLoss: 0.768293\n","Train Epoch: 3 [1280/1423 (89%)]\tLoss: 1.279225\n","Epoch 3 - Training: Average loss: 0.8681, Accuracy: 82.57%\n","Evaluation set: Average loss: 1.9366, Accuracy: 527/743 (70.93%), F1-score: 0.6953\n","\n","Train Epoch: 4 [0/1423 (0%)]\tLoss: 0.410537\n","Train Epoch: 4 [320/1423 (22%)]\tLoss: 0.945658\n","Train Epoch: 4 [640/1423 (44%)]\tLoss: 0.913683\n","Train Epoch: 4 [960/1423 (67%)]\tLoss: 0.297784\n","Train Epoch: 4 [1280/1423 (89%)]\tLoss: 0.849126\n","Epoch 4 - Training: Average loss: 0.7524, Accuracy: 84.05%\n","Evaluation set: Average loss: 2.2805, Accuracy: 498/743 (67.03%), F1-score: 0.6404\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 5 [0/1423 (0%)]\tLoss: 0.144494\n","Train Epoch: 5 [320/1423 (22%)]\tLoss: 0.499421\n","Train Epoch: 5 [640/1423 (44%)]\tLoss: 1.171066\n","Train Epoch: 5 [960/1423 (67%)]\tLoss: 1.392666\n","Train Epoch: 5 [1280/1423 (89%)]\tLoss: 0.782134\n","Epoch 5 - Training: Average loss: 0.6320, Accuracy: 85.66%\n","Evaluation set: Average loss: 1.6858, Accuracy: 533/743 (71.74%), F1-score: 0.6899\n","\n","Train Epoch: 6 [0/1423 (0%)]\tLoss: 0.493439\n","Train Epoch: 6 [320/1423 (22%)]\tLoss: 0.941901\n","Train Epoch: 6 [640/1423 (44%)]\tLoss: 1.074108\n","Train Epoch: 6 [960/1423 (67%)]\tLoss: 0.272179\n","Train Epoch: 6 [1280/1423 (89%)]\tLoss: 0.222192\n","Epoch 6 - Training: Average loss: 0.5288, Accuracy: 87.00%\n","Evaluation set: Average loss: 1.7008, Accuracy: 514/743 (69.18%), F1-score: 0.6720\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 7 [0/1423 (0%)]\tLoss: 0.264230\n","Train Epoch: 7 [320/1423 (22%)]\tLoss: 0.294237\n","Train Epoch: 7 [640/1423 (44%)]\tLoss: 0.531388\n","Train Epoch: 7 [960/1423 (67%)]\tLoss: 0.414084\n","Train Epoch: 7 [1280/1423 (89%)]\tLoss: 0.309475\n","Epoch 7 - Training: Average loss: 0.4877, Accuracy: 87.35%\n","Evaluation set: Average loss: 1.4687, Accuracy: 519/743 (69.85%), F1-score: 0.6811\n","\n","Train Epoch: 8 [0/1423 (0%)]\tLoss: 0.172000\n","Train Epoch: 8 [320/1423 (22%)]\tLoss: 0.095200\n","Train Epoch: 8 [640/1423 (44%)]\tLoss: 0.654960\n","Train Epoch: 8 [960/1423 (67%)]\tLoss: 0.377833\n","Train Epoch: 8 [1280/1423 (89%)]\tLoss: 0.151644\n","Epoch 8 - Training: Average loss: 0.3633, Accuracy: 89.04%\n","Evaluation set: Average loss: 1.7217, Accuracy: 521/743 (70.12%), F1-score: 0.6803\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 9 [0/1423 (0%)]\tLoss: 0.218472\n","Train Epoch: 9 [320/1423 (22%)]\tLoss: 0.579906\n","Train Epoch: 9 [640/1423 (44%)]\tLoss: 0.313497\n","Train Epoch: 9 [960/1423 (67%)]\tLoss: 0.018620\n","Train Epoch: 9 [1280/1423 (89%)]\tLoss: 0.344659\n","Epoch 9 - Training: Average loss: 0.3349, Accuracy: 90.44%\n","Evaluation set: Average loss: 1.4994, Accuracy: 523/743 (70.39%), F1-score: 0.6732\n","\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 10 [0/1423 (0%)]\tLoss: 0.350403\n","Train Epoch: 10 [320/1423 (22%)]\tLoss: 0.208040\n","Train Epoch: 10 [640/1423 (44%)]\tLoss: 0.552861\n","Train Epoch: 10 [960/1423 (67%)]\tLoss: 0.296567\n","Train Epoch: 10 [1280/1423 (89%)]\tLoss: 0.391493\n","Epoch 10 - Training: Average loss: 0.2879, Accuracy: 89.95%\n","Evaluation set: Average loss: 1.6367, Accuracy: 521/743 (70.12%), F1-score: 0.6811\n","\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 11 [0/1423 (0%)]\tLoss: 0.788591\n","Train Epoch: 11 [320/1423 (22%)]\tLoss: 0.052166\n","Train Epoch: 11 [640/1423 (44%)]\tLoss: 0.402725\n","Train Epoch: 11 [960/1423 (67%)]\tLoss: 0.523985\n","Train Epoch: 11 [1280/1423 (89%)]\tLoss: 0.466432\n","Epoch 11 - Training: Average loss: 0.3757, Accuracy: 90.30%\n","Evaluation set: Average loss: 1.7932, Accuracy: 520/743 (69.99%), F1-score: 0.6637\n","\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 12 [0/1423 (0%)]\tLoss: 0.304277\n","Train Epoch: 12 [320/1423 (22%)]\tLoss: 0.147900\n","Train Epoch: 12 [640/1423 (44%)]\tLoss: 0.135026\n","Train Epoch: 12 [960/1423 (67%)]\tLoss: 0.242722\n","Train Epoch: 12 [1280/1423 (89%)]\tLoss: 0.151534\n","Epoch 12 - Training: Average loss: 0.2530, Accuracy: 92.48%\n","Evaluation set: Average loss: 1.3804, Accuracy: 532/743 (71.60%), F1-score: 0.6912\n","\n","Train Epoch: 13 [0/1423 (0%)]\tLoss: 0.161904\n","Train Epoch: 13 [320/1423 (22%)]\tLoss: 0.309600\n","Train Epoch: 13 [640/1423 (44%)]\tLoss: 0.103777\n","Train Epoch: 13 [960/1423 (67%)]\tLoss: 0.051580\n","Train Epoch: 13 [1280/1423 (89%)]\tLoss: 0.125085\n","Epoch 13 - Training: Average loss: 0.1510, Accuracy: 94.52%\n","Evaluation set: Average loss: 1.3761, Accuracy: 534/743 (71.87%), F1-score: 0.6988\n","\n","Train Epoch: 14 [0/1423 (0%)]\tLoss: 0.059046\n","Train Epoch: 14 [320/1423 (22%)]\tLoss: 0.163438\n","Train Epoch: 14 [640/1423 (44%)]\tLoss: 0.032656\n","Train Epoch: 14 [960/1423 (67%)]\tLoss: 0.128291\n","Train Epoch: 14 [1280/1423 (89%)]\tLoss: 0.026874\n","Epoch 14 - Training: Average loss: 0.1188, Accuracy: 96.06%\n","Evaluation set: Average loss: 1.3045, Accuracy: 539/743 (72.54%), F1-score: 0.7075\n","\n","Train Epoch: 15 [0/1423 (0%)]\tLoss: 0.023697\n","Train Epoch: 15 [320/1423 (22%)]\tLoss: 0.073549\n","Train Epoch: 15 [640/1423 (44%)]\tLoss: 0.020752\n","Train Epoch: 15 [960/1423 (67%)]\tLoss: 0.022360\n","Train Epoch: 15 [1280/1423 (89%)]\tLoss: 0.076202\n","Epoch 15 - Training: Average loss: 0.1352, Accuracy: 95.92%\n","Evaluation set: Average loss: 1.2926, Accuracy: 541/743 (72.81%), F1-score: 0.7146\n","\n","Train Epoch: 16 [0/1423 (0%)]\tLoss: 0.042937\n","Train Epoch: 16 [320/1423 (22%)]\tLoss: 0.117364\n","Train Epoch: 16 [640/1423 (44%)]\tLoss: 0.076314\n","Train Epoch: 16 [960/1423 (67%)]\tLoss: 0.059544\n","Train Epoch: 16 [1280/1423 (89%)]\tLoss: 0.159446\n","Epoch 16 - Training: Average loss: 0.1239, Accuracy: 95.78%\n","Evaluation set: Average loss: 1.2534, Accuracy: 543/743 (73.08%), F1-score: 0.7144\n","\n","Train Epoch: 17 [0/1423 (0%)]\tLoss: 0.204469\n","Train Epoch: 17 [320/1423 (22%)]\tLoss: 0.066010\n","Train Epoch: 17 [640/1423 (44%)]\tLoss: 0.088909\n","Train Epoch: 17 [960/1423 (67%)]\tLoss: 0.028077\n","Train Epoch: 17 [1280/1423 (89%)]\tLoss: 0.251576\n","Epoch 17 - Training: Average loss: 0.1093, Accuracy: 95.50%\n","Evaluation set: Average loss: 1.2376, Accuracy: 540/743 (72.68%), F1-score: 0.7093\n","\n","Train Epoch: 18 [0/1423 (0%)]\tLoss: 0.043628\n","Train Epoch: 18 [320/1423 (22%)]\tLoss: 0.277705\n","Train Epoch: 18 [640/1423 (44%)]\tLoss: 0.159446\n","Train Epoch: 18 [960/1423 (67%)]\tLoss: 0.082633\n","Train Epoch: 18 [1280/1423 (89%)]\tLoss: 0.078129\n","Epoch 18 - Training: Average loss: 0.1110, Accuracy: 96.28%\n","Evaluation set: Average loss: 1.2777, Accuracy: 532/743 (71.60%), F1-score: 0.6977\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 19 [0/1423 (0%)]\tLoss: 0.028548\n","Train Epoch: 19 [320/1423 (22%)]\tLoss: 0.218021\n","Train Epoch: 19 [640/1423 (44%)]\tLoss: 0.329733\n","Train Epoch: 19 [960/1423 (67%)]\tLoss: 0.037694\n","Train Epoch: 19 [1280/1423 (89%)]\tLoss: 0.043616\n","Epoch 19 - Training: Average loss: 0.0805, Accuracy: 97.68%\n","Evaluation set: Average loss: 1.2205, Accuracy: 544/743 (73.22%), F1-score: 0.7161\n","\n","Train Epoch: 20 [0/1423 (0%)]\tLoss: 0.021680\n","Train Epoch: 20 [320/1423 (22%)]\tLoss: 0.198605\n","Train Epoch: 20 [640/1423 (44%)]\tLoss: 0.056518\n","Train Epoch: 20 [960/1423 (67%)]\tLoss: 0.007875\n","Train Epoch: 20 [1280/1423 (89%)]\tLoss: 0.088483\n","Epoch 20 - Training: Average loss: 0.0966, Accuracy: 96.63%\n","Evaluation set: Average loss: 1.2468, Accuracy: 545/743 (73.35%), F1-score: 0.7194\n","\n","EarlyStopping counter: 1 out of 10\n","Fold 2 is starting ...\n","Validation subjects : ['subject101' 'subject108']\n","Train subjects: ['subject104' 'subject105' 'subject106' 'subject107']\n","Train Epoch: 1 [0/1457 (0%)]\tLoss: 9.978580\n","Train Epoch: 1 [320/1457 (22%)]\tLoss: 4.892262\n","Train Epoch: 1 [640/1457 (43%)]\tLoss: 4.707328\n","Train Epoch: 1 [960/1457 (65%)]\tLoss: 1.909997\n","Train Epoch: 1 [1280/1457 (87%)]\tLoss: 1.779327\n","Epoch 1 - Training: Average loss: 3.1738, Accuracy: 62.73%\n","Evaluation set: Average loss: 2.4000, Accuracy: 478/709 (67.42%), F1-score: 0.6473\n","\n","Train Epoch: 2 [0/1457 (0%)]\tLoss: 1.912977\n","Train Epoch: 2 [320/1457 (22%)]\tLoss: 1.253323\n","Train Epoch: 2 [640/1457 (43%)]\tLoss: 0.769728\n","Train Epoch: 2 [960/1457 (65%)]\tLoss: 1.392763\n","Train Epoch: 2 [1280/1457 (87%)]\tLoss: 0.971464\n","Epoch 2 - Training: Average loss: 1.1120, Accuracy: 79.62%\n","Evaluation set: Average loss: 2.1306, Accuracy: 457/709 (64.46%), F1-score: 0.6074\n","\n","Train Epoch: 3 [0/1457 (0%)]\tLoss: 0.802461\n","Train Epoch: 3 [320/1457 (22%)]\tLoss: 1.275555\n","Train Epoch: 3 [640/1457 (43%)]\tLoss: 0.877936\n","Train Epoch: 3 [960/1457 (65%)]\tLoss: 0.596658\n","Train Epoch: 3 [1280/1457 (87%)]\tLoss: 0.680168\n","Epoch 3 - Training: Average loss: 0.7739, Accuracy: 82.36%\n","Evaluation set: Average loss: 1.9332, Accuracy: 474/709 (66.85%), F1-score: 0.6257\n","\n","Train Epoch: 4 [0/1457 (0%)]\tLoss: 0.348980\n","Train Epoch: 4 [320/1457 (22%)]\tLoss: 0.322552\n","Train Epoch: 4 [640/1457 (43%)]\tLoss: 0.431120\n","Train Epoch: 4 [960/1457 (65%)]\tLoss: 0.470465\n","Train Epoch: 4 [1280/1457 (87%)]\tLoss: 0.273549\n","Epoch 4 - Training: Average loss: 0.4834, Accuracy: 86.82%\n","Evaluation set: Average loss: 1.6496, Accuracy: 472/709 (66.57%), F1-score: 0.6301\n","\n","Train Epoch: 5 [0/1457 (0%)]\tLoss: 0.501462\n","Train Epoch: 5 [320/1457 (22%)]\tLoss: 0.619731\n","Train Epoch: 5 [640/1457 (43%)]\tLoss: 0.306680\n","Train Epoch: 5 [960/1457 (65%)]\tLoss: 0.608085\n","Train Epoch: 5 [1280/1457 (87%)]\tLoss: 0.704101\n","Epoch 5 - Training: Average loss: 0.4210, Accuracy: 86.55%\n","Evaluation set: Average loss: 1.9493, Accuracy: 468/709 (66.01%), F1-score: 0.6136\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 6 [0/1457 (0%)]\tLoss: 0.236742\n","Train Epoch: 6 [320/1457 (22%)]\tLoss: 0.299377\n","Train Epoch: 6 [640/1457 (43%)]\tLoss: 1.042078\n","Train Epoch: 6 [960/1457 (65%)]\tLoss: 0.437178\n","Train Epoch: 6 [1280/1457 (87%)]\tLoss: 0.525639\n","Epoch 6 - Training: Average loss: 0.4818, Accuracy: 86.20%\n","Evaluation set: Average loss: 2.1810, Accuracy: 481/709 (67.84%), F1-score: 0.6395\n","\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 7 [0/1457 (0%)]\tLoss: 0.447175\n","Train Epoch: 7 [320/1457 (22%)]\tLoss: 0.623497\n","Train Epoch: 7 [640/1457 (43%)]\tLoss: 0.397054\n","Train Epoch: 7 [960/1457 (65%)]\tLoss: 0.912885\n","Train Epoch: 7 [1280/1457 (87%)]\tLoss: 0.162787\n","Epoch 7 - Training: Average loss: 0.4969, Accuracy: 86.82%\n","Evaluation set: Average loss: 1.9924, Accuracy: 489/709 (68.97%), F1-score: 0.6595\n","\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 8 [0/1457 (0%)]\tLoss: 0.331278\n","Train Epoch: 8 [320/1457 (22%)]\tLoss: 0.124282\n","Train Epoch: 8 [640/1457 (43%)]\tLoss: 0.183193\n","Train Epoch: 8 [960/1457 (65%)]\tLoss: 0.722261\n","Train Epoch: 8 [1280/1457 (87%)]\tLoss: 0.663653\n","Epoch 8 - Training: Average loss: 0.4635, Accuracy: 87.85%\n","Evaluation set: Average loss: 1.5634, Accuracy: 499/709 (70.38%), F1-score: 0.6798\n","\n","Train Epoch: 9 [0/1457 (0%)]\tLoss: 0.126357\n","Train Epoch: 9 [320/1457 (22%)]\tLoss: 0.406520\n","Train Epoch: 9 [640/1457 (43%)]\tLoss: 0.457774\n","Train Epoch: 9 [960/1457 (65%)]\tLoss: 0.300356\n","Train Epoch: 9 [1280/1457 (87%)]\tLoss: 0.334791\n","Epoch 9 - Training: Average loss: 0.2883, Accuracy: 90.46%\n","Evaluation set: Average loss: 1.9777, Accuracy: 466/709 (65.73%), F1-score: 0.6273\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 10 [0/1457 (0%)]\tLoss: 0.407530\n","Train Epoch: 10 [320/1457 (22%)]\tLoss: 0.305802\n","Train Epoch: 10 [640/1457 (43%)]\tLoss: 0.246705\n","Train Epoch: 10 [960/1457 (65%)]\tLoss: 0.469674\n","Train Epoch: 10 [1280/1457 (87%)]\tLoss: 0.303451\n","Epoch 10 - Training: Average loss: 0.3512, Accuracy: 89.91%\n","Evaluation set: Average loss: 2.1141, Accuracy: 499/709 (70.38%), F1-score: 0.6602\n","\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 11 [0/1457 (0%)]\tLoss: 0.045935\n","Train Epoch: 11 [320/1457 (22%)]\tLoss: 0.357216\n","Train Epoch: 11 [640/1457 (43%)]\tLoss: 0.293388\n","Train Epoch: 11 [960/1457 (65%)]\tLoss: 0.174312\n","Train Epoch: 11 [1280/1457 (87%)]\tLoss: 0.535787\n","Epoch 11 - Training: Average loss: 0.3749, Accuracy: 89.29%\n","Evaluation set: Average loss: 1.8490, Accuracy: 479/709 (67.56%), F1-score: 0.6389\n","\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 12 [0/1457 (0%)]\tLoss: 0.642561\n","Train Epoch: 12 [320/1457 (22%)]\tLoss: 0.343191\n","Train Epoch: 12 [640/1457 (43%)]\tLoss: 0.205393\n","Train Epoch: 12 [960/1457 (65%)]\tLoss: 0.444201\n","Train Epoch: 12 [1280/1457 (87%)]\tLoss: 0.261325\n","Epoch 12 - Training: Average loss: 0.3197, Accuracy: 90.87%\n","Evaluation set: Average loss: 1.8373, Accuracy: 501/709 (70.66%), F1-score: 0.6768\n","\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 13 [0/1457 (0%)]\tLoss: 0.227092\n","Train Epoch: 13 [320/1457 (22%)]\tLoss: 0.393062\n","Train Epoch: 13 [640/1457 (43%)]\tLoss: 0.503055\n","Train Epoch: 13 [960/1457 (65%)]\tLoss: 0.171816\n","Train Epoch: 13 [1280/1457 (87%)]\tLoss: 0.404214\n","Epoch 13 - Training: Average loss: 0.2416, Accuracy: 91.83%\n","Evaluation set: Average loss: 1.6920, Accuracy: 492/709 (69.39%), F1-score: 0.6716\n","\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 14 [0/1457 (0%)]\tLoss: 0.143512\n","Train Epoch: 14 [320/1457 (22%)]\tLoss: 0.122699\n","Train Epoch: 14 [640/1457 (43%)]\tLoss: 0.019438\n","Train Epoch: 14 [960/1457 (65%)]\tLoss: 0.018247\n","Train Epoch: 14 [1280/1457 (87%)]\tLoss: 0.009929\n","Epoch 14 - Training: Average loss: 0.1211, Accuracy: 95.47%\n","Evaluation set: Average loss: 1.6215, Accuracy: 504/709 (71.09%), F1-score: 0.6854\n","\n","EarlyStopping counter: 6 out of 10\n","Train Epoch: 15 [0/1457 (0%)]\tLoss: 0.097983\n","Train Epoch: 15 [320/1457 (22%)]\tLoss: 0.263271\n","Train Epoch: 15 [640/1457 (43%)]\tLoss: 0.166303\n","Train Epoch: 15 [960/1457 (65%)]\tLoss: 0.063141\n","Train Epoch: 15 [1280/1457 (87%)]\tLoss: 0.037721\n","Epoch 15 - Training: Average loss: 0.1087, Accuracy: 96.36%\n","Evaluation set: Average loss: 1.6421, Accuracy: 506/709 (71.37%), F1-score: 0.6904\n","\n","EarlyStopping counter: 7 out of 10\n","Train Epoch: 16 [0/1457 (0%)]\tLoss: 0.034375\n","Train Epoch: 16 [320/1457 (22%)]\tLoss: 0.153007\n","Train Epoch: 16 [640/1457 (43%)]\tLoss: 0.079087\n","Train Epoch: 16 [960/1457 (65%)]\tLoss: 0.015813\n","Train Epoch: 16 [1280/1457 (87%)]\tLoss: 0.076119\n","Epoch 16 - Training: Average loss: 0.1086, Accuracy: 95.95%\n","Evaluation set: Average loss: 1.6072, Accuracy: 504/709 (71.09%), F1-score: 0.6850\n","\n","EarlyStopping counter: 8 out of 10\n","Train Epoch: 17 [0/1457 (0%)]\tLoss: 0.096632\n","Train Epoch: 17 [320/1457 (22%)]\tLoss: 0.037498\n","Train Epoch: 17 [640/1457 (43%)]\tLoss: 0.175746\n","Train Epoch: 17 [960/1457 (65%)]\tLoss: 0.637175\n","Train Epoch: 17 [1280/1457 (87%)]\tLoss: 0.083837\n","Epoch 17 - Training: Average loss: 0.1346, Accuracy: 95.81%\n","Evaluation set: Average loss: 1.5848, Accuracy: 508/709 (71.65%), F1-score: 0.6885\n","\n","EarlyStopping counter: 9 out of 10\n","Train Epoch: 18 [0/1457 (0%)]\tLoss: 0.024143\n","Train Epoch: 18 [320/1457 (22%)]\tLoss: 0.086303\n","Train Epoch: 18 [640/1457 (43%)]\tLoss: 0.051901\n","Train Epoch: 18 [960/1457 (65%)]\tLoss: 0.063334\n","Train Epoch: 18 [1280/1457 (87%)]\tLoss: 0.053758\n","Epoch 18 - Training: Average loss: 0.1128, Accuracy: 95.61%\n","Evaluation set: Average loss: 1.5958, Accuracy: 501/709 (70.66%), F1-score: 0.6794\n","\n","EarlyStopping counter: 10 out of 10\n","Early stopping\n","Fold 3 is starting ...\n","Validation subjects : ['subject106' 'subject107']\n","Train subjects: ['subject101' 'subject104' 'subject105' 'subject108']\n","Train Epoch: 1 [0/1452 (0%)]\tLoss: 9.474906\n","Train Epoch: 1 [320/1452 (22%)]\tLoss: 2.834030\n","Train Epoch: 1 [640/1452 (43%)]\tLoss: 4.195367\n","Train Epoch: 1 [960/1452 (65%)]\tLoss: 2.785599\n","Train Epoch: 1 [1280/1452 (87%)]\tLoss: 0.914261\n","Epoch 1 - Training: Average loss: 2.9032, Accuracy: 62.26%\n","Evaluation set: Average loss: 3.3588, Accuracy: 507/714 (71.01%), F1-score: 0.6176\n","\n","Train Epoch: 2 [0/1452 (0%)]\tLoss: 1.560350\n","Train Epoch: 2 [320/1452 (22%)]\tLoss: 1.855415\n","Train Epoch: 2 [640/1452 (43%)]\tLoss: 0.649887\n","Train Epoch: 2 [960/1452 (65%)]\tLoss: 1.040566\n","Train Epoch: 2 [1280/1452 (87%)]\tLoss: 1.854023\n","Epoch 2 - Training: Average loss: 1.1945, Accuracy: 76.86%\n","Evaluation set: Average loss: 2.7325, Accuracy: 474/714 (66.39%), F1-score: 0.6133\n","\n","Train Epoch: 3 [0/1452 (0%)]\tLoss: 3.042510\n","Train Epoch: 3 [320/1452 (22%)]\tLoss: 0.504421\n","Train Epoch: 3 [640/1452 (43%)]\tLoss: 1.461231\n","Train Epoch: 3 [960/1452 (65%)]\tLoss: 0.556126\n","Train Epoch: 3 [1280/1452 (87%)]\tLoss: 0.301396\n","Epoch 3 - Training: Average loss: 0.9153, Accuracy: 80.03%\n","Evaluation set: Average loss: 1.2076, Accuracy: 542/714 (75.91%), F1-score: 0.7179\n","\n","Train Epoch: 4 [0/1452 (0%)]\tLoss: 0.307635\n","Train Epoch: 4 [320/1452 (22%)]\tLoss: 0.675174\n","Train Epoch: 4 [640/1452 (43%)]\tLoss: 0.808949\n","Train Epoch: 4 [960/1452 (65%)]\tLoss: 0.611461\n","Train Epoch: 4 [1280/1452 (87%)]\tLoss: 1.472103\n","Epoch 4 - Training: Average loss: 0.6956, Accuracy: 81.89%\n","Evaluation set: Average loss: 1.1286, Accuracy: 534/714 (74.79%), F1-score: 0.7121\n","\n","Train Epoch: 5 [0/1452 (0%)]\tLoss: 0.664608\n","Train Epoch: 5 [320/1452 (22%)]\tLoss: 0.354017\n","Train Epoch: 5 [640/1452 (43%)]\tLoss: 0.476638\n","Train Epoch: 5 [960/1452 (65%)]\tLoss: 0.409295\n","Train Epoch: 5 [1280/1452 (87%)]\tLoss: 0.541237\n","Epoch 5 - Training: Average loss: 0.4444, Accuracy: 85.40%\n","Evaluation set: Average loss: 1.1447, Accuracy: 538/714 (75.35%), F1-score: 0.7077\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 6 [0/1452 (0%)]\tLoss: 0.811007\n","Train Epoch: 6 [320/1452 (22%)]\tLoss: 0.183137\n","Train Epoch: 6 [640/1452 (43%)]\tLoss: 0.304133\n","Train Epoch: 6 [960/1452 (65%)]\tLoss: 0.216274\n","Train Epoch: 6 [1280/1452 (87%)]\tLoss: 0.171183\n","Epoch 6 - Training: Average loss: 0.5299, Accuracy: 86.36%\n","Evaluation set: Average loss: 1.4692, Accuracy: 527/714 (73.81%), F1-score: 0.6969\n","\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 7 [0/1452 (0%)]\tLoss: 0.487699\n","Train Epoch: 7 [320/1452 (22%)]\tLoss: 0.633586\n","Train Epoch: 7 [640/1452 (43%)]\tLoss: 0.460569\n","Train Epoch: 7 [960/1452 (65%)]\tLoss: 0.372469\n","Train Epoch: 7 [1280/1452 (87%)]\tLoss: 0.586628\n","Epoch 7 - Training: Average loss: 0.4419, Accuracy: 86.23%\n","Evaluation set: Average loss: 2.0009, Accuracy: 488/714 (68.35%), F1-score: 0.6282\n","\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 8 [0/1452 (0%)]\tLoss: 0.479714\n","Train Epoch: 8 [320/1452 (22%)]\tLoss: 0.750715\n","Train Epoch: 8 [640/1452 (43%)]\tLoss: 0.967854\n","Train Epoch: 8 [960/1452 (65%)]\tLoss: 0.142719\n","Train Epoch: 8 [1280/1452 (87%)]\tLoss: 0.619707\n","Epoch 8 - Training: Average loss: 0.4943, Accuracy: 86.71%\n","Evaluation set: Average loss: 1.4801, Accuracy: 527/714 (73.81%), F1-score: 0.6766\n","\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 9 [0/1452 (0%)]\tLoss: 0.845104\n","Train Epoch: 9 [320/1452 (22%)]\tLoss: 0.145177\n","Train Epoch: 9 [640/1452 (43%)]\tLoss: 0.127305\n","Train Epoch: 9 [960/1452 (65%)]\tLoss: 0.059215\n","Train Epoch: 9 [1280/1452 (87%)]\tLoss: 0.412210\n","Epoch 9 - Training: Average loss: 0.2441, Accuracy: 91.74%\n","Evaluation set: Average loss: 1.1781, Accuracy: 552/714 (77.31%), F1-score: 0.7393\n","\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 10 [0/1452 (0%)]\tLoss: 0.114253\n","Train Epoch: 10 [320/1452 (22%)]\tLoss: 0.564132\n","Train Epoch: 10 [640/1452 (43%)]\tLoss: 0.138547\n","Train Epoch: 10 [960/1452 (65%)]\tLoss: 0.186555\n","Train Epoch: 10 [1280/1452 (87%)]\tLoss: 0.105502\n","Epoch 10 - Training: Average loss: 0.1919, Accuracy: 93.32%\n","Evaluation set: Average loss: 1.1928, Accuracy: 546/714 (76.47%), F1-score: 0.7220\n","\n","EarlyStopping counter: 6 out of 10\n","Train Epoch: 11 [0/1452 (0%)]\tLoss: 0.055052\n","Train Epoch: 11 [320/1452 (22%)]\tLoss: 0.053858\n","Train Epoch: 11 [640/1452 (43%)]\tLoss: 0.115821\n","Train Epoch: 11 [960/1452 (65%)]\tLoss: 0.312282\n","Train Epoch: 11 [1280/1452 (87%)]\tLoss: 0.141631\n","Epoch 11 - Training: Average loss: 0.1733, Accuracy: 94.21%\n","Evaluation set: Average loss: 1.1980, Accuracy: 535/714 (74.93%), F1-score: 0.7137\n","\n","EarlyStopping counter: 7 out of 10\n","Train Epoch: 12 [0/1452 (0%)]\tLoss: 0.119946\n","Train Epoch: 12 [320/1452 (22%)]\tLoss: 0.114632\n","Train Epoch: 12 [640/1452 (43%)]\tLoss: 0.175674\n","Train Epoch: 12 [960/1452 (65%)]\tLoss: 0.151717\n","Train Epoch: 12 [1280/1452 (87%)]\tLoss: 0.128604\n","Epoch 12 - Training: Average loss: 0.1696, Accuracy: 94.15%\n","Evaluation set: Average loss: 1.1013, Accuracy: 543/714 (76.05%), F1-score: 0.7244\n","\n","Train Epoch: 13 [0/1452 (0%)]\tLoss: 0.042486\n","Train Epoch: 13 [320/1452 (22%)]\tLoss: 0.127077\n","Train Epoch: 13 [640/1452 (43%)]\tLoss: 0.072133\n","Train Epoch: 13 [960/1452 (65%)]\tLoss: 0.185193\n","Train Epoch: 13 [1280/1452 (87%)]\tLoss: 0.030563\n","Epoch 13 - Training: Average loss: 0.1484, Accuracy: 94.77%\n","Evaluation set: Average loss: 1.1513, Accuracy: 533/714 (74.65%), F1-score: 0.7054\n","\n","EarlyStopping counter: 1 out of 10\n","Train Epoch: 14 [0/1452 (0%)]\tLoss: 0.264600\n","Train Epoch: 14 [320/1452 (22%)]\tLoss: 0.060050\n","Train Epoch: 14 [640/1452 (43%)]\tLoss: 0.071007\n","Train Epoch: 14 [960/1452 (65%)]\tLoss: 0.049345\n","Train Epoch: 14 [1280/1452 (87%)]\tLoss: 0.015085\n","Epoch 14 - Training: Average loss: 0.1481, Accuracy: 94.63%\n","Evaluation set: Average loss: 1.1381, Accuracy: 539/714 (75.49%), F1-score: 0.7122\n","\n","EarlyStopping counter: 2 out of 10\n","Train Epoch: 15 [0/1452 (0%)]\tLoss: 0.040183\n","Train Epoch: 15 [320/1452 (22%)]\tLoss: 0.101792\n","Train Epoch: 15 [640/1452 (43%)]\tLoss: 0.095194\n","Train Epoch: 15 [960/1452 (65%)]\tLoss: 0.113854\n","Train Epoch: 15 [1280/1452 (87%)]\tLoss: 0.107682\n","Epoch 15 - Training: Average loss: 0.1111, Accuracy: 96.69%\n","Evaluation set: Average loss: 1.1564, Accuracy: 537/714 (75.21%), F1-score: 0.7113\n","\n","EarlyStopping counter: 3 out of 10\n","Train Epoch: 16 [0/1452 (0%)]\tLoss: 0.165429\n","Train Epoch: 16 [320/1452 (22%)]\tLoss: 0.190906\n","Train Epoch: 16 [640/1452 (43%)]\tLoss: 0.037710\n","Train Epoch: 16 [960/1452 (65%)]\tLoss: 0.131872\n","Train Epoch: 16 [1280/1452 (87%)]\tLoss: 0.316706\n","Epoch 16 - Training: Average loss: 0.1469, Accuracy: 94.35%\n","Evaluation set: Average loss: 1.2406, Accuracy: 529/714 (74.09%), F1-score: 0.6890\n","\n","EarlyStopping counter: 4 out of 10\n","Train Epoch: 17 [0/1452 (0%)]\tLoss: 0.117442\n","Train Epoch: 17 [320/1452 (22%)]\tLoss: 0.241739\n","Train Epoch: 17 [640/1452 (43%)]\tLoss: 0.162180\n","Train Epoch: 17 [960/1452 (65%)]\tLoss: 0.089707\n","Train Epoch: 17 [1280/1452 (87%)]\tLoss: 0.043433\n","Epoch 17 - Training: Average loss: 0.1161, Accuracy: 96.07%\n","Evaluation set: Average loss: 1.2068, Accuracy: 535/714 (74.93%), F1-score: 0.6984\n","\n","EarlyStopping counter: 5 out of 10\n","Train Epoch: 18 [0/1452 (0%)]\tLoss: 0.056736\n","Train Epoch: 18 [320/1452 (22%)]\tLoss: 0.097159\n","Train Epoch: 18 [640/1452 (43%)]\tLoss: 0.035347\n","Train Epoch: 18 [960/1452 (65%)]\tLoss: 0.084781\n","Train Epoch: 18 [1280/1452 (87%)]\tLoss: 0.082275\n","Epoch 18 - Training: Average loss: 0.1152, Accuracy: 95.52%\n","Evaluation set: Average loss: 1.2235, Accuracy: 533/714 (74.65%), F1-score: 0.6943\n","\n","EarlyStopping counter: 6 out of 10\n","Train Epoch: 19 [0/1452 (0%)]\tLoss: 0.133586\n","Train Epoch: 19 [320/1452 (22%)]\tLoss: 0.042856\n","Train Epoch: 19 [640/1452 (43%)]\tLoss: 0.090384\n","Train Epoch: 19 [960/1452 (65%)]\tLoss: 0.079318\n","Train Epoch: 19 [1280/1452 (87%)]\tLoss: 0.016359\n","Epoch 19 - Training: Average loss: 0.1142, Accuracy: 96.01%\n","Evaluation set: Average loss: 1.1707, Accuracy: 528/714 (73.95%), F1-score: 0.6934\n","\n","EarlyStopping counter: 7 out of 10\n","Train Epoch: 20 [0/1452 (0%)]\tLoss: 0.157112\n","Train Epoch: 20 [320/1452 (22%)]\tLoss: 0.161150\n","Train Epoch: 20 [640/1452 (43%)]\tLoss: 0.191865\n","Train Epoch: 20 [960/1452 (65%)]\tLoss: 0.052617\n","Train Epoch: 20 [1280/1452 (87%)]\tLoss: 0.111422\n","Epoch 20 - Training: Average loss: 0.1351, Accuracy: 96.14%\n","Evaluation set: Average loss: 1.3093, Accuracy: 536/714 (75.07%), F1-score: 0.7008\n","\n","EarlyStopping counter: 8 out of 10\n"]}]},{"cell_type":"markdown","source":["## EVALUATION On TEST DATA (SUBJECT 5 & SUBJECT 1) No DP"],"metadata":{"id":"xZBygvGnXQtT"}},{"cell_type":"code","source":["import torch\n","import gc\n","import os\n","\n","# Enable detailed CUDA error reporting for debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","# Clear CUDA cache first\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","final_model_path = f'attack_results_noDP/final_models/{final_model_version}'\n","final_scaler_path = f'attack_results_noDP/final_models/{final_scaler_version}'\n","final_label_encoder_path = f'attack_results_noDP/final_models/{final_label_encoder_version}'\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","try:\n","    original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')\n","    final_test_model = copy.deepcopy(original_model)\n","\n","    # CRITICAL FIX: Load to CPU first, then move to device\n","    print(\"Loading model state dict...\")\n","    state_dict = torch.load(final_model_path, weights_only=True, map_location='cpu')\n","    print(\"Applying state dict...\")\n","    final_test_model.load_state_dict(remove_module_prefix(state_dict))\n","    print(\"Moving model to device...\")\n","    final_test_model.to(device)\n","\n","    print(\"Loading scaler and label encoder...\")\n","    final_scaler_test = pickle.load(open(final_scaler_path, 'rb'))\n","    final_le_test = pickle.load(open(final_label_encoder_path, 'rb'))\n","\n","    print(\"Preparing test data...\")\n","    X_test_mia = np.array(mia_test_X)\n","    y_test_mia = np.array(mia_test_y)\n","\n","    encoded_y_test_mia = final_le_test.transform(y_test_mia)\n","\n","    # Debug info\n","    print(f\"Original test labels: {np.unique(y_test_mia)}\")\n","    print(f\"Encoded test labels: {np.unique(encoded_y_test_mia)}\")\n","    print(f\"Label range: {np.min(encoded_y_test_mia)} to {np.max(encoded_y_test_mia)}\")\n","    print(f\"N_CLASSES: {N_CLASSES}\")\n","\n","    scaled_X_test_mia = final_scaler_test.transform(X_test_mia)\n","    mia_test_data_loader = get_data_loader(scaled_X_test_mia, encoded_y_test_mia, BATCH_SIZE=BATCH_SIZE_LIST[0], shuffle=False)\n","\n","    print(\"Running evaluation...\")\n","    final_test_loss, final_test_acc, final_test_f1 = evaluate(final_test_model, mia_test_data_loader, criterion, device)\n","\n","    print(f'Final Model Test Loss: {final_test_loss}, Test Acc: {final_test_acc}, Test F1: {final_test_f1}')\n","\n","except RuntimeError as e:\n","    print(f\"CUDA Error occurred: {e}\")\n","    print(\"Attempting CPU fallback...\")\n","\n","    # Fallback to CPU\n","    device_cpu = torch.device('cpu')\n","\n","    original_model = get_pretrained_harnet(class_num=N_CLASSES, model_name='harnet10')\n","    final_test_model = copy.deepcopy(original_model)\n","\n","    state_dict = torch.load(final_model_path, weights_only=True, map_location='cpu')\n","    final_test_model.load_state_dict(remove_module_prefix(state_dict))\n","    final_test_model.to(device_cpu)  # Use CPU\n","\n","    final_scaler_test = pickle.load(open(final_scaler_path, 'rb'))\n","    final_le_test = pickle.load(open(final_label_encoder_path, 'rb'))\n","\n","    X_test_mia = np.array(mia_test_X)\n","    y_test_mia = np.array(mia_test_y)\n","    encoded_y_test_mia = final_le_test.transform(y_test_mia)\n","    scaled_X_test_mia = final_scaler_test.transform(X_test_mia)\n","\n","    mia_test_data_loader = get_data_loader(scaled_X_test_mia, encoded_y_test_mia, BATCH_SIZE=BATCH_SIZE_LIST[0], shuffle=False)\n","\n","    final_test_loss, final_test_acc, final_test_f1 = evaluate(final_test_model, mia_test_data_loader, criterion, device_cpu)\n","\n","    print(f'Final Model Test Loss (CPU): {final_test_loss}, Test Acc: {final_test_acc}, Test F1: {final_test_f1}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIEnIVCfW7jX","executionInfo":{"status":"ok","timestamp":1750894937170,"user_tz":-120,"elapsed":11177,"user":{"displayName":"Ozan GÜNER","userId":"08667159980350915066"}},"outputId":"f09a4234-8eeb-4e4c-dcc8-014230148f0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/OxWearables/ssl-wearables/zipball/main\" to /root/.cache/torch/hub/main.zip\n"]},{"output_type":"stream","name":"stdout","text":["131 Weights loaded\n","Loading model state dict...\n","Applying state dict...\n","Moving model to device...\n","Loading scaler and label encoder...\n","Preparing test data...\n","Original test labels: [ 1  2  3  4 12 13 16 17]\n","Encoded test labels: [0 1 2 3 4 5 6 7]\n","Label range: 0 to 7\n","N_CLASSES: 8\n","Running evaluation...\n","Evaluation set: Average loss: 1.3445, Accuracy: 523/721 (72.54%), F1-score: 0.7021\n","\n","Final Model Test Loss: 1.344465445486529, Test Acc: 72.5381414701803, Test F1: 0.7021015853978003\n"]}]}]}